{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22c0e553-f6ab-440f-b90f-ff3aaaadf838",
   "metadata": {},
   "source": [
    "## Projeto de Predição de doenças cardiacas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e0207c-73cb-4aa3-a649-951163bf70e5",
   "metadata": {},
   "source": [
    "### Instalando Livrarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "951115ae-0157-4786-b983-b63d57bfee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pyspark\n",
    "# !pip install -q handyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd118385-9ceb-4feb-a03c-146acefa751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "import urllib.request\n",
    "from pyspark.sql.functions import col, count, isnan, when, round\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, Imputer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, UnivariateFeatureSelector\n",
    "from pyspark.ml.evaluation import  BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454fc655-25f5-49e9-b382-33845812afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae85d8a-18eb-40e6-920a-5ecea9182fe6",
   "metadata": {},
   "source": [
    "### Criando a Sessão em Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41b2875c-c996-493c-b6a9-9156e08fbfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Session\n",
    "conf = SparkConf() \\\n",
    "    .set(\"spark.executor.instances\", \"2\") \\\n",
    "    .set(\"spark.executor.memory\", \"2g\") \\\n",
    "    .set(\"spark.driver.memory\", \"2g\") \\\n",
    "    .setAppName(\"MeuAPP\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .config(conf=conf) \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Obter o contexto Spark da sessão\n",
    "# sc = spark.sparkContext\n",
    "\n",
    "# # Parar a sessão Spark quando não for mais necessária\n",
    "# spark.stop()\n",
    "\n",
    "# # Parar o contexto Spark quando não for mais necessário\n",
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23fcad95-909c-4676-8577-b65d9c32e066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session Spark:\n",
      " - ID da aplicação: local-1711583145961\n",
      " - Nome da aplicação: MeuAPP\n"
     ]
    }
   ],
   "source": [
    "# Imprimir informações sobre a sessão Spark\n",
    "print(\"Session Spark:\")\n",
    "print(\" - ID da aplicação:\", spark.sparkContext.applicationId)\n",
    "print(\" - Nome da aplicação:\", spark.sparkContext.appName)\n",
    "\n",
    "# Imprimir informações sobre o contexto Spark\n",
    "# print(\"\\nContexto Spark:\")\n",
    "# print(\" - Versão do Spark:\", sc.version)\n",
    "# print(\" - Modo de execução:\", sc.master)\n",
    "\n",
    "# # Print the Python version of SparkContext\n",
    "# print(\"\\nThe Python version of Spark Context in the PySpark shell is\", sc.pythonVer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59cad283-6fa5-41ab-a6b8-5b73e1262e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://af9d95f152c7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MeuAPP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7e8e7d70f5d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e78029-b322-4ed6-8fb6-5aaf57337fb7",
   "metadata": {},
   "source": [
    "### Importando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1a94478-c09d-45ec-b936-d3af742ddc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esquema dos Dados:\n",
      "root\n",
      " |-- hotel: string (nullable = true)\n",
      " |-- is_canceled: integer (nullable = true)\n",
      " |-- lead_time: integer (nullable = true)\n",
      " |-- arrival_date_year: integer (nullable = true)\n",
      " |-- arrival_date_month: string (nullable = true)\n",
      " |-- arrival_date_week_number: integer (nullable = true)\n",
      " |-- arrival_date_day_of_month: integer (nullable = true)\n",
      " |-- stays_in_weekend_nights: integer (nullable = true)\n",
      " |-- stays_in_week_nights: integer (nullable = true)\n",
      " |-- adults: integer (nullable = true)\n",
      " |-- children: double (nullable = true)\n",
      " |-- babies: integer (nullable = true)\n",
      " |-- meal: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- market_segment: string (nullable = true)\n",
      " |-- distribution_channel: string (nullable = true)\n",
      " |-- is_repeated_guest: integer (nullable = true)\n",
      " |-- previous_cancellations: integer (nullable = true)\n",
      " |-- previous_bookings_not_canceled: integer (nullable = true)\n",
      " |-- reserved_room_type: string (nullable = true)\n",
      " |-- assigned_room_type: string (nullable = true)\n",
      " |-- booking_changes: integer (nullable = true)\n",
      " |-- deposit_type: string (nullable = true)\n",
      " |-- agent: double (nullable = true)\n",
      " |-- company: double (nullable = true)\n",
      " |-- days_in_waiting_list: integer (nullable = true)\n",
      " |-- customer_type: string (nullable = true)\n",
      " |-- adr: double (nullable = true)\n",
      " |-- required_car_parking_spaces: integer (nullable = true)\n",
      " |-- total_of_special_requests: integer (nullable = true)\n",
      " |-- reservation_status: string (nullable = true)\n",
      " |-- reservation_status_date: date (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- phone-number: string (nullable = true)\n",
      " |-- credit_card: string (nullable = true)\n",
      "\n",
      "Shape of the dataset:  (119390, 36)\n",
      "\n",
      "Primeiras 5 linhas dos Dados:\n",
      "+------------+-----------+---------+-----------------+------------------+------------------------+-------------------------+-----------------------+--------------------+------+--------+------+----+-------+--------------+--------------------+-----------------+----------------------+------------------------------+------------------+------------------+---------------+------------+-----+-------+--------------------+-------------+----+---------------------------+-------------------------+------------------+-----------------------+--------------+--------------------+------------+----------------+\n",
      "|       hotel|is_canceled|lead_time|arrival_date_year|arrival_date_month|arrival_date_week_number|arrival_date_day_of_month|stays_in_weekend_nights|stays_in_week_nights|adults|children|babies|meal|country|market_segment|distribution_channel|is_repeated_guest|previous_cancellations|previous_bookings_not_canceled|reserved_room_type|assigned_room_type|booking_changes|deposit_type|agent|company|days_in_waiting_list|customer_type| adr|required_car_parking_spaces|total_of_special_requests|reservation_status|reservation_status_date|          name|               email|phone-number|     credit_card|\n",
      "+------------+-----------+---------+-----------------+------------------+------------------------+-------------------------+-----------------------+--------------------+------+--------+------+----+-------+--------------+--------------------+-----------------+----------------------+------------------------------+------------------+------------------+---------------+------------+-----+-------+--------------------+-------------+----+---------------------------+-------------------------+------------------+-----------------------+--------------+--------------------+------------+----------------+\n",
      "|Resort Hotel|          0|      342|             2015|              July|                      27|                        1|                      0|                   0|     2|     0.0|     0|  BB|    PRT|        Direct|              Direct|                0|                     0|                             0|                 C|                 C|              3|  No Deposit| NULL|   NULL|                   0|    Transient| 0.0|                          0|                        0|         Check-Out|             2015-07-01| Ernest Barnes|Ernest.Barnes31@o...|669-792-1661|************4322|\n",
      "|Resort Hotel|          0|      737|             2015|              July|                      27|                        1|                      0|                   0|     2|     0.0|     0|  BB|    PRT|        Direct|              Direct|                0|                     0|                             0|                 C|                 C|              4|  No Deposit| NULL|   NULL|                   0|    Transient| 0.0|                          0|                        0|         Check-Out|             2015-07-01|  Andrea Baker|Andrea_Baker94@ao...|858-637-6955|************9157|\n",
      "|Resort Hotel|          0|        7|             2015|              July|                      27|                        1|                      0|                   1|     1|     0.0|     0|  BB|    GBR|        Direct|              Direct|                0|                     0|                             0|                 A|                 C|              0|  No Deposit| NULL|   NULL|                   0|    Transient|75.0|                          0|                        0|         Check-Out|             2015-07-02|Rebecca Parker|Rebecca_Parker@co...|652-885-2745|************3734|\n",
      "|Resort Hotel|          0|       13|             2015|              July|                      27|                        1|                      0|                   1|     1|     0.0|     0|  BB|    GBR|     Corporate|           Corporate|                0|                     0|                             0|                 A|                 A|              0|  No Deposit|304.0|   NULL|                   0|    Transient|75.0|                          0|                        0|         Check-Out|             2015-07-02|  Laura Murray|   Laura_M@gmail.com|364-656-8427|************5677|\n",
      "|Resort Hotel|          0|       14|             2015|              July|                      27|                        1|                      0|                   2|     2|     0.0|     0|  BB|    GBR|     Online TA|               TA/TO|                0|                     0|                             0|                 A|                 A|              0|  No Deposit|240.0|   NULL|                   0|    Transient|98.0|                          0|                        1|         Check-Out|             2015-07-03|   Linda Hines|  LHines@verizon.com|713-226-5883|************5498|\n",
      "+------------+-----------+---------+-----------------+------------------+------------------------+-------------------------+-----------------------+--------------------+------+--------+------+----+-------+--------------+--------------------+-----------------+----------------------+------------------------------+------------------+------------------+---------------+------------+-----+-------+--------------------+-------------+----+---------------------------+-------------------------+------------------+-----------------------+--------------+--------------------+------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Link para o conjunto de dados Iris\n",
    "# link_dados = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
    "# local_path = \"./data/raw/heard_disease_raw_data.csv\"  # Caminho local onde o arquivo será salvo\n",
    "\n",
    "# # Baixar o arquivo CSV do URL\n",
    "# urllib.request.urlretrieve(link_dados, local_path)\n",
    "\n",
    "# # Nomes colunas\n",
    "# nomes_colunas=[\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \n",
    "#                 \"restecg\", \"thalach\", \"exang\", \"oldpeak\",\n",
    "#                 \"slope\",\"ca\",\"thal\",\"num\"\n",
    "#               ]\n",
    "\n",
    "local_path = \"./data/raw/hotel_booking.csv\" \n",
    "# work/data/raw/hotel_booking.csv\n",
    "# Carregar os dados como um DataFrame Spark\n",
    "df = spark.read.csv(local_path, header=True, inferSchema=True)\n",
    "\n",
    "# Atribuir nomes às colunas\n",
    "# df = df.toDF()\n",
    "\n",
    "# Exibir o esquema dos dados\n",
    "print(\"Esquema dos Dados:\")\n",
    "df.printSchema()\n",
    "\n",
    "# Exibir a quantidade de colunas e linhas\n",
    "print(\"Shape of the dataset: \", (df.count(), len(df.columns)))\n",
    "\n",
    "# Exibir as primeiras 5 linhas dos dados\n",
    "print(\"\\nPrimeiras 5 linhas dos Dados:\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c8870b6-6198-4bba-a475-25155e63d92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47554ec9-c7a3-4b93-bbae-62a0c9b520dc",
   "metadata": {},
   "source": [
    "### Processamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c70ed674-24d0-46b6-b215-d5c52d4ffe3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|is_canceled|count|\n",
      "+-----------+-----+\n",
      "|          1|44224|\n",
      "|          0|75166|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('is_canceled').agg(count(\"*\").alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8743a63-f557-430f-849a-0ad42b324e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('reservation_status_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec593435-164b-4a97-8619-2861c93d78da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----+-------+\n",
      "|children|country|agent|company|\n",
      "+--------+-------+-----+-------+\n",
      "|       4|    488|16340| 112593|\n",
      "+--------+-------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_df = df.select([count(when(\n",
    "                                col(c).contains('None') | \\\n",
    "                                col(c).contains('NULL') | \\\n",
    "                                (col(c) == '') | \\\n",
    "                                col(c).isNull() | \\\n",
    "                                isnan(col(c)), c ))\n",
    "           .alias(c)\n",
    "           for c in df.columns])\n",
    "\n",
    "# expresoes_condicionais = [when (col(coluna) >0,coluna).alias(coluna) for coluna in df.columns ]\n",
    "colunas_com_soma_maior_que_1 = [coluna for coluna in null_df.columns if null_df.select(coluna).collect()[0][0] > 1]\n",
    "\n",
    "null_df.select(*colunas_com_soma_maior_que_1).show()\n",
    "# null_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a021e4da-16b0-47cb-b8c0-a6e381fc1534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['children', 'country', 'agent', 'company']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colunas_com_soma_maior_que_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54fd18b1-f6db-4271-b1f4-9471901ca95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(*colunas_com_soma_maior_que_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9375b417-192e-400f-877f-844de7b706cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hotel: string (nullable = true)\n",
      " |-- is_canceled: integer (nullable = true)\n",
      " |-- lead_time: integer (nullable = true)\n",
      " |-- arrival_date_year: integer (nullable = true)\n",
      " |-- arrival_date_month: string (nullable = true)\n",
      " |-- arrival_date_week_number: integer (nullable = true)\n",
      " |-- arrival_date_day_of_month: integer (nullable = true)\n",
      " |-- stays_in_weekend_nights: integer (nullable = true)\n",
      " |-- stays_in_week_nights: integer (nullable = true)\n",
      " |-- adults: integer (nullable = true)\n",
      " |-- babies: integer (nullable = true)\n",
      " |-- meal: string (nullable = true)\n",
      " |-- market_segment: string (nullable = true)\n",
      " |-- distribution_channel: string (nullable = true)\n",
      " |-- is_repeated_guest: integer (nullable = true)\n",
      " |-- previous_cancellations: integer (nullable = true)\n",
      " |-- previous_bookings_not_canceled: integer (nullable = true)\n",
      " |-- reserved_room_type: string (nullable = true)\n",
      " |-- assigned_room_type: string (nullable = true)\n",
      " |-- booking_changes: integer (nullable = true)\n",
      " |-- deposit_type: string (nullable = true)\n",
      " |-- days_in_waiting_list: integer (nullable = true)\n",
      " |-- customer_type: string (nullable = true)\n",
      " |-- adr: double (nullable = true)\n",
      " |-- required_car_parking_spaces: integer (nullable = true)\n",
      " |-- total_of_special_requests: integer (nullable = true)\n",
      " |-- reservation_status: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- phone-number: string (nullable = true)\n",
      " |-- credit_card: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea0b43c7-6c2f-452c-9c23-886c1f75ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletando linhas com erro\n",
    "# df_clean = df.filter(col(\"ca\")!=\"?\").filter(col(\"thal\")!='?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf6a0a2b-9f3e-42a2-8f79-fe73ee174d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd registros: 119390\n",
      "Qtd colunas: 31\n"
     ]
    }
   ],
   "source": [
    "print(f\"Qtd registros: {df.count()}\")\n",
    "print(f\"Qtd colunas: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a20dc37-b624-4097-a611-78ece0b43d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----------+\n",
      "|         Nome_Column|NUnicos|Repre_Total|\n",
      "+--------------------+-------+-----------+\n",
      "|   is_repeated_guest|      2|        0.0|\n",
      "|               hotel|      2|        0.0|\n",
      "|         is_canceled|      2|        0.0|\n",
      "|   arrival_date_year|      3|        0.0|\n",
      "|  reservation_status|      3|        0.0|\n",
      "|        deposit_type|      3|        0.0|\n",
      "|       customer_type|      4|        0.0|\n",
      "|              babies|      5|        0.0|\n",
      "|distribution_channel|      5|        0.0|\n",
      "|                meal|      5|        0.0|\n",
      "|required_car_park...|      5|        0.0|\n",
      "|total_of_special_...|      6|       0.01|\n",
      "|      market_segment|      8|       0.01|\n",
      "|  reserved_room_type|     10|       0.01|\n",
      "|  arrival_date_month|     12|       0.01|\n",
      "|  assigned_room_type|     12|       0.01|\n",
      "|              adults|     14|       0.01|\n",
      "|previous_cancella...|     15|       0.01|\n",
      "|stays_in_weekend_...|     17|       0.01|\n",
      "|     booking_changes|     21|       0.02|\n",
      "|arrival_date_day_...|     31|       0.03|\n",
      "|stays_in_week_nights|     35|       0.03|\n",
      "|arrival_date_week...|     53|       0.04|\n",
      "|previous_bookings...|     73|       0.06|\n",
      "|days_in_waiting_list|    128|       0.11|\n",
      "|           lead_time|    479|        0.4|\n",
      "|                 adr|   8879|       7.44|\n",
      "|         credit_card|   9000|       7.54|\n",
      "|                name|  81503|      68.27|\n",
      "|               email| 115889|      97.07|\n",
      "|        phone-number| 119390|      100.0|\n",
      "+--------------------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_uniques = spark.createDataFrame([(i, len(df.select(i).distinct().collect())) for i  in df.columns],\n",
    "                      ['Nome_Column', 'NUnicos'])\\\n",
    "                        .orderBy(['NUnicos'],ascending=True)\\\n",
    "                        .withColumn('Repre_Total',round((col('NUnicos')/df.count())*100,2))\n",
    "df_uniques.show(31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "126edbf8-82b1-4183-bec2-ed79022bca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colunas con valores distintos e valores constantes\n",
    "col_valores_dispersos = ['credit_card' ,'name' ,'email' ,'phone-number', 'adr']\n",
    "df_clean = df.drop(*col_valores_dispersos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "102cd0e9-14ea-4903-9a37-d068e393a846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|is_canceled|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('is_canceled')).distinct().show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f25cb187-2e33-452b-ad4b-9ecdfa6d2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRocando o nom da label\n",
    "df_clean=df_clean.withColumnRenamed('is_canceled', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7034a7b-2c2b-4559-b435-b39b5f998a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(hotel='Resort Hotel', label=0, lead_time=342, arrival_date_year=2015, arrival_date_month='July', arrival_date_week_number=27, arrival_date_day_of_month=1, stays_in_weekend_nights=0, stays_in_week_nights=0, adults=2, babies=0, meal='BB', market_segment='Direct', distribution_channel='Direct', is_repeated_guest=0, previous_cancellations=0, previous_bookings_not_canceled=0, reserved_room_type='C', assigned_room_type='C', booking_changes=3, deposit_type='No Deposit', days_in_waiting_list=0, customer_type='Transient', required_car_parking_spaces=0, total_of_special_requests=0, reservation_status='Check-Out')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0cf9a76a-a22e-4739-b6eb-85cd1ca2c7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a label\n",
    "df_clean = df_clean.withColumn('label', col('label').cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbd0bcd-55c5-409a-b7c1-cbc414deaa6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bce3ad8-cde0-4611-99da-78785dac10ee",
   "metadata": {},
   "source": [
    "### Feature enginnering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dff6d051-9898-4b89-a93a-ed4b1642fc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do conjunto de treino: 83594\n",
      "Tamanho do conjunto de teste: 35796\n"
     ]
    }
   ],
   "source": [
    "# Divida o DataFrame em 70% para treino e 30% para teste\n",
    "train_df, test_df = df_clean.randomSplit([0.7, 0.3],seed=24)\n",
    "\n",
    "# Imprima o tamanho de cada subconjunto\n",
    "print(\"Tamanho do conjunto de treino:\", train_df.count())\n",
    "print(\"Tamanho do conjunto de teste:\", test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fdca3a91-ef98-4128-893b-48c6b9a79cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+\n",
      "|label|count|porcent|\n",
      "+-----+-----+-------+\n",
      "|  0.0|52695| 0.6304|\n",
      "|  1.0|30899| 0.3696|\n",
      "+-----+-----+-------+\n",
      "\n",
      "+-----+-----+-------+\n",
      "|label|count|porcent|\n",
      "+-----+-----+-------+\n",
      "|  0.0|22471| 0.6278|\n",
      "|  1.0|13325| 0.3722|\n",
      "+-----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.groupBy('label').count().withColumn('porcent', round(col('count')/train_df.count(),4)).show()\n",
    "test_df.groupBy('label').count().withColumn('porcent', round(col('count')/test_df.count(),4)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8b4e930-fef5-426b-8d70-ca2105d213b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crianndo Class weight // Para os dados de treino\n",
    "# Conta o número de ocorrências de cada classe\n",
    "class_counts = train_df.groupBy('label').count().collect()\n",
    "# Classes no dataframe\n",
    "unique_classes = [row[0] for row in class_counts]\n",
    "# qtd Clases \n",
    "qtd_classes = len(unique_classes)\n",
    "# Qtd de registros por Classe\n",
    "class_counts = [row[1] for row in class_counts]\n",
    "# Calcula o número total de amostras no dataframe\n",
    "total_samples = train_df.count()\n",
    "\n",
    "# Calcula os pesos de classe\n",
    "class_weights = {}\n",
    "for class_label, class_count in zip(unique_classes, class_counts):\n",
    "    class_weight = total_samples / (qtd_classes* class_count)\n",
    "    class_weights[class_label] = class_weight\n",
    "\n",
    "# Criando uma coluna ao Dataframe\n",
    "train_df_w = train_df.withColumn('weight',when(col('label') == 0.0, class_weights[0.0])\\\n",
    "                    .when(col('label') == 1.0, class_weights[1.0])\\\n",
    "                    .otherwise(None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b8aa68-0907-4026-999c-2dd3bc04e67e",
   "metadata": {},
   "source": [
    "### Criando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec431ac7-8f13-4466-b504-0015d30c29cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import sample_by\n",
    "\n",
    "# # Definindo a coluna de data\n",
    "# data_column = \"data\"\n",
    "\n",
    "# # Definindo a proporção de divisão (80% treino, 20% teste)\n",
    "# train_ratio = 0.8\n",
    "\n",
    "# # Amostragem estratificada por data\n",
    "# df_train = df.sample_by(data_column, fractions={train_ratio: \"train\", 1-train_ratio: \"test\"})\n",
    "\n",
    "# # Separando os conjuntos de treino e teste\n",
    "# X_train = df_train.filter(df_train.split == \"train\")\n",
    "# X_test = df_train.filter(df_train.split == \"test\")\n",
    "\n",
    "# # Extraindo as labels\n",
    "# y_train = X_train[target_column]\n",
    "# y_test = X_test[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4aaded7-d86a-4972-8ead-b9f17c499565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28cec73e-3137-435b-b914-9f21dcd8c1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_trained = pipeline.fit(df_clean)\n",
    "# df_trannsformed = pipeline_trained.transform(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb538c23-59e5-46b1-ac2a-0b80a6f7ecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colunas_trannsformadas = [encoder.getOutputCol() for encoder in encoders]+imputer.getOutputCols()\n",
    "# df_trannsformed.select(*colunas_trannsformadas, col(assembler.getOutputCol()), col(label)).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b4caadf-5357-4e0c-94b1-d8bfdeced547",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'label'\n",
    "weight = 'weight'\n",
    "colunas_categoricas = [coluna for coluna, tipo in df_clean.dtypes if tipo in ['string'] and coluna not in (label,weight)]\n",
    "colunas_numericas = [coluna for coluna, tipo in df_clean.dtypes if tipo in ['int', 'double'] and coluna not in (label,weight)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0eb5db24-1f10-4e19-a53c-937eff7309b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificação Colunas:\n",
      "Colunas no dataset: 27\n",
      "Colunas nas listas: 27\n"
     ]
    }
   ],
   "source": [
    "# qtd_colunas = \n",
    "# qtd_colunas_em listas=\n",
    "print(f\"\"\"Verificação Colunas:\n",
    "Colunas no dataset: {len(train_df_w.columns)}\n",
    "Colunas nas listas: {len(colunas_categoricas) + len(colunas_numericas) + len([label,weight])}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "afa883b2-6c6e-4e69-8496-026359991dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "# Imputer for numeric columns\n",
    "imputer = Imputer(inputCols=colunas_numericas,\n",
    "                  outputCols=[f\"{c}_imputed\" for c in colunas_numericas],\n",
    "                  strategy= 'mean',\n",
    "                  missingValue= -666,\n",
    "                 )\n",
    "stages.append(imputer)\n",
    "\n",
    "# StringIndexer for categorical columns\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_indexed\", \n",
    "                          handleInvalid='keep',\n",
    "                          stringOrderType='frequencyDesc'\n",
    "                         ) for c in colunas_categoricas]\n",
    "stages += indexers\n",
    "\n",
    "# OneHotEncoder for indexed categorical columns\n",
    "encoders = [OneHotEncoder(inputCol=indexer.getOutputCol(), \n",
    "                          outputCol=f\"{indexer.getOutputCol()}_encoded\",\n",
    "                          handleInvalid = 'keep',\n",
    "                          dropLast=False\n",
    "                         ) for indexer in indexers]\n",
    "stages += encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "720bfd76-50d4-4dfc-a8b4-a1b9b9815850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar VectorAssembler para combinar todas as variáveis\n",
    "assembler = VectorAssembler(inputCols=imputer.getOutputCols()+[encoder.getOutputCol() for encoder in encoders]  #[f\"{c}_encoded\" for c in colunas_categoricas]\n",
    "                            , outputCol=\"features_raw\")\n",
    "stages.append(assembler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05a7a625-a9e9-4478-a955-9c99c6d4bc40",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Criar UnivariateFeatureSelector para selecionar as melhores características\n",
    "selector = UnivariateFeatureSelector()\\\n",
    "                  .setFeatureType(\"continuous\")\\\n",
    "                  .setLabelType(\"categorical\")\\\n",
    "                  .setSelectionMode(\"percentile\")\\\n",
    "                  .setFeaturesCol(\"features_raw\")\\\n",
    "                  .setLabelCol(\"label\")\\\n",
    "                  .setOutputCol(\"features\")\n",
    "                  # .setSelectionThreshold(0.50)\n",
    "                    # SelectorType (tipo de teste estatistico\n",
    "\n",
    "stages.append(selector)\n",
    "# .setSelectionThreshold(1)\n",
    "\n",
    "\n",
    "\n",
    "# featuresCol=\"features_raw\"\n",
    "#                                      , labelCol=\"label\"\n",
    "#                                      # ,selectionMode='percentile'\n",
    "#                                      # ,selectionThreshold=0.8\n",
    "#                                      # , selectionMethod=\"chi2\"\n",
    "#                                      , outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0830bb7a-2093-42e3-a778-7f56d22b0e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [2, 5, 10]) \\\n",
    "    .addGrid(gbt.maxIter, [10, 20]) \\\n",
    "    .addGrid(gbt.stepSize, [0.1]) \\\n",
    "    .addGrid(selector.selectionThreshold, [0.70, 0.90]) \\\n",
    "    .build()\n",
    "# .addGrid(gbt.weightCol, ['weight', \"None\"]) \\\n",
    "pipeline = Pipeline(stages= [imputer]+indexers+encoders+[assembler, selector, gbt])\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "\n",
    "# Criar CrossValidator\n",
    "cv = CrossValidator(estimator=pipeline, \n",
    "                    estimatorParamMaps=paramGrid, \n",
    "                    evaluator=evaluator, \n",
    "                    numFolds=5,\n",
    "                    parallelism=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "294176f4-c66e-48e2-86a5-582717c2e7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar o modelo e avaliar o desempenho\n",
    "cv_model = cv.fit(train_df_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1523ec4f-848f-4542-8815-eeb05692f281",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='GBTClassifier_d5714ef9e724', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 2, Param(parent='GBTClassifier_d5714ef9e724', name='maxIter', doc='max number of iterations (>= 0).'): 10, Param(parent='GBTClassifier_d5714ef9e724', name='stepSize', doc='Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator.'): 0.1, Param(parent='UnivariateFeatureSelector_45833190c056', name='selectionThreshold', doc='The upper bound of the features that selector will select.'): 0.7}\n",
      "{Param(parent='GBTClassifier_d5714ef9e724', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 2, Param(parent='GBTClassifier_d5714ef9e724', name='maxIter', doc='max number of iterations (>= 0).'): 10, Param(parent='GBTClassifier_d5714ef9e724', name='stepSize', doc='Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator.'): 0.1, Param(parent='UnivariateFeatureSelector_45833190c056', name='selectionThreshold', doc='The upper bound of the features that selector will select.'): 0.9}\n",
      "{Param(parent='GBTClassifier_d5714ef9e724', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 2, Param(parent='GBTClassifier_d5714ef9e724', name='maxIter', doc='max number of iterations (>= 0).'): 20, Param(parent='GBTClassifier_d5714ef9e724', name='stepSize', doc='Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator.'): 0.1, Param(parent='UnivariateFeatureSelector_45833190c056', name='selectionThreshold', doc='The upper bound of the features that selector will select.'): 0.7}\n",
      "{Param(parent='GBTClassifier_d5714ef9e724', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 2, Param(parent='GBTClassifier_d5714ef9e724', name='maxIter', doc='max number of iterations (>= 0).'): 20, Param(parent='GBTClassifier_d5714ef9e724', name='stepSize', doc='Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator.'): 0.1, Param(parent='UnivariateFeatureSelector_45833190c056', name='selectionThreshold', doc='The upper bound of the features that selector will select.'): 0.9}\n",
      "{Param(parent='GBTClassifier_d5714ef9e724', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5, Param(parent='GBTClassifier_d5714ef9e724', name='maxIter', doc='max number of iterations (>= 0).'): 10, Param(parent='GBTClassifier_d5714ef9e724', name='stepSize', doc='Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator.'): 0.1, Param(parent='UnivariateFeatureSelector_45833190c056', name='selectionThreshold', doc='The upper bound of the features that selector will select.'): 0.7}\n",
      "{Param(parent='GBTClassifier_d5714ef9e724', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5, Param(parent='GBTClassifier_d5714ef9e724', name='maxIter', doc='max number of iterations (>= 0).'): 10, Param(parent='GBTClassifier_d5714ef9e724', name='stepSize', doc='Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator.'): 0.1, Param(parent='UnivariateFeatureSelector_45833190c056', name='selectionThreshold', doc='The upper bound of the features that selector will select.'): 0.9}\n",
      "{Param(parent='GBTClassifier_d5714ef9e724', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5, Param(parent='GBTClassifier_d5714ef9e724', name='maxIter', doc='max number of iterations (>= 0).'): 20, Param(parent='GBTClassifier_d5714ef9e724', name='stepSize', doc='Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator.'): 0.1, Param(parent='UnivariateFeatureSelector_45833190c056', name='selectionThreshold', doc='The upper bound of the features that selector will select.'): 0.7}\n",
      "{Param(parent='GBTClassifier_d5714ef9e724', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5, Param(parent='GBTClassifier_d5714ef9e724', name='maxIter', doc='max number of iterations (>= 0).'): 20, Param(parent='GBTClassifier_d5714ef9e724', name='stepSize', doc='Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator.'): 0.1, Param(parent='UnivariateFeatureSelector_45833190c056', name='selectionThreshold', doc='The upper bound of the features that selector will select.'): 0.9}\n",
      "{Param(parent='GBTClassifier_d5714ef9e724', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 10, Param(parent='GBTClassifier_d5714ef9e724', name='maxIter', doc='max number of iterations (>= 0).'): 10, Param(parent='GBTClassifier_d5714ef9e724', name='stepSize', doc='Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator.'): 0.1, Param(parent='UnivariateFeatureSelector_45833190c056', name='selectionThreshold', doc='The upper bound of the features that selector will select.'): 0.7}\n",
      "{Param(parent='GBTClassifier_d5714ef9e724', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 10, Param(parent='GBTClassifier_d5714ef9e724', name='maxIter', doc='max number of iterations (>= 0).'): 10, Param(parent='GBTClassifier_d5714ef9e724', name='stepSize', doc='Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator.'): 0.1, Param(parent='UnivariateFeatureSelector_45833190c056', name='selectionThreshold', doc='The upper bound of the features that selector will select.'): 0.9}\n",
      "{Param(parent='GBTClassifier_d5714ef9e724', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 10, Param(parent='GBTClassifier_d5714ef9e724', name='maxIter', doc='max number of iterations (>= 0).'): 20, Param(parent='GBTClassifier_d5714ef9e724', name='stepSize', doc='Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator.'): 0.1, Param(parent='UnivariateFeatureSelector_45833190c056', name='selectionThreshold', doc='The upper bound of the features that selector will select.'): 0.7}\n",
      "{Param(parent='GBTClassifier_d5714ef9e724', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 10, Param(parent='GBTClassifier_d5714ef9e724', name='maxIter', doc='max number of iterations (>= 0).'): 20, Param(parent='GBTClassifier_d5714ef9e724', name='stepSize', doc='Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator.'): 0.1, Param(parent='UnivariateFeatureSelector_45833190c056', name='selectionThreshold', doc='The upper bound of the features that selector will select.'): 0.9}\n"
     ]
    }
   ],
   "source": [
    "# Lista de parametros para cada interação do grid\n",
    "for params in paramGrid:\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a41ec-f482-4f91-bff8-a68f7a1230cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tentativa de utilização de tdqm para seguimento do processo \n",
    "from tqdm import tqdm\n",
    "\n",
    "# Criando a barra de progresso\n",
    "with tqdm(total=len(paramGrid)) as pbar:\n",
    "  for params in paramGrid:\n",
    "    # Treinamento e avaliação do pipeline\n",
    "    cv = CrossValidator(estimator=pipeline, \n",
    "                        estimatorParamMaps=params, \n",
    "                        evaluator=evaluator, \n",
    "                        numFolds=5)\n",
    "      \n",
    "    cv_model = cv.fit(df_clean)\n",
    "\n",
    "    # Atualizando a barra de progresso\n",
    "    pbar.update()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af4791a-d10b-432d-b6fa-860a68c4d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlflow\n",
    "\n",
    "# # Registrando os parâmetros do pipeline\n",
    "# with mlflow.start_run():\n",
    "#   cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "#   ccv_model = cv.fit(df_clean)\n",
    "\n",
    "# # Visualizando os parâmetros no dashboard\n",
    "# mlflow.ui.launch_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a1a57ae3-eed3-433b-890f-df1a3184fd37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_ab04e7d1ae1f"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2 maneiras de acessar os parametros:\n",
    "# bestModel\n",
    "best_model = cv_model.bestModel\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e62c109e-d4c2-46ea-8bef-062d23a5cbb9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='GBTClassifier_d5714ef9e724', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval.'): False, Param(parent='GBTClassifier_d5714ef9e724', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext.'): 10, Param(parent='GBTClassifier_d5714ef9e724', name='featureSubsetStrategy', doc=\"The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto'\"): 'all', Param(parent='GBTClassifier_d5714ef9e724', name='featuresCol', doc='features column name.'): 'features', Param(parent='GBTClassifier_d5714ef9e724', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: variance'): 'variance', Param(parent='GBTClassifier_d5714ef9e724', name='labelCol', doc='label column name.'): 'label', Param(parent='GBTClassifier_d5714ef9e724', name='leafCol', doc='Leaf indices column name. Predicted leaf index of each instance in each tree by preorder.'): '', Param(parent='GBTClassifier_d5714ef9e724', name='lossType', doc='Loss function which GBT tries to minimize (case-insensitive). Supported options: logistic'): 'logistic', Param(parent='GBTClassifier_d5714ef9e724', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 32, Param(parent='GBTClassifier_d5714ef9e724', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 2, Param(parent='GBTClassifier_d5714ef9e724', name='maxIter', doc='max number of iterations (>= 0).'): 10, Param(parent='GBTClassifier_d5714ef9e724', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size.'): 256, Param(parent='GBTClassifier_d5714ef9e724', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0, Param(parent='GBTClassifier_d5714ef9e724', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1, Param(parent='GBTClassifier_d5714ef9e724', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0, Param(parent='GBTClassifier_d5714ef9e724', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='GBTClassifier_d5714ef9e724', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability', Param(parent='GBTClassifier_d5714ef9e724', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction', Param(parent='GBTClassifier_d5714ef9e724', name='seed', doc='random seed.'): 24, Param(parent='GBTClassifier_d5714ef9e724', name='stepSize', doc='Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator.'): 0.1, Param(parent='GBTClassifier_d5714ef9e724', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0, Param(parent='GBTClassifier_d5714ef9e724', name='validationTol', doc='Threshold for stopping early when fit with validation is used. If the error rate on the validation input changes by less than the validationTol, then learning will stop early (before `maxIter`). This parameter is ignored when fit without validation is used.'): 0.01, Param(parent='GBTClassifier_d5714ef9e724', name='weightCol', doc='weight column name. If this is not set or empty, we treat all instance weights as 1.0.'): 'weight'}\n"
     ]
    }
   ],
   "source": [
    "# Acessando o melhor modelo de regressão linear dentro do pipeline\n",
    "melhor_modelo_gbt = best_model.stages[-1]\n",
    "\n",
    "# Imprimindo os parâmetros do melhor modelo do GBT\n",
    "print(melhor_modelo_gbt.extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c8a56fd5-b23f-4460-a024-73da0d24569e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='UnivariateFeatureSelector_45833190c056', name='featuresCol', doc='features column name.'): 'features_raw',\n",
       " Param(parent='UnivariateFeatureSelector_45833190c056', name='labelCol', doc='label column name.'): 'label',\n",
       " Param(parent='UnivariateFeatureSelector_45833190c056', name='outputCol', doc='output column name.'): 'features',\n",
       " Param(parent='UnivariateFeatureSelector_45833190c056', name='selectionMode', doc='The selection mode. Supported options: numTopFeatures (default), percentile, fpr, fdr, fwe.'): 'percentile',\n",
       " Param(parent='UnivariateFeatureSelector_45833190c056', name='featureType', doc='The feature type. Supported options: categorical, continuous.'): 'continuous',\n",
       " Param(parent='UnivariateFeatureSelector_45833190c056', name='labelType', doc='The label type. Supported options: categorical, continuous.'): 'categorical',\n",
       " Param(parent='UnivariateFeatureSelector_45833190c056', name='selectionThreshold', doc='The upper bound of the features that selector will select.'): 0.7}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imprimindo os parametros do melhor  Seletor de variaveis\n",
    "best_model.stages[-2].extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9fbab9e7-5322-4cde-9c27-b978f03259fb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(hotel='Resort Hotel', label=0.0, lead_time=0, arrival_date_year=2015, arrival_date_month='August', arrival_date_week_number=32, arrival_date_day_of_month=4, stays_in_weekend_nights=0, stays_in_week_nights=1, adults=1, babies=0, meal='BB', market_segment='Direct', distribution_channel='Direct', is_repeated_guest=0, previous_cancellations=0, previous_bookings_not_canceled=0, reserved_room_type='F', assigned_room_type='F', booking_changes=0, deposit_type='No Deposit', days_in_waiting_list=0, customer_type='Transient', required_car_parking_spaces=0, total_of_special_requests=0, reservation_status='Check-Out', lead_time_imputed=0, arrival_date_year_imputed=2015, arrival_date_week_number_imputed=32, arrival_date_day_of_month_imputed=4, stays_in_weekend_nights_imputed=0, stays_in_week_nights_imputed=1, adults_imputed=1, babies_imputed=0, is_repeated_guest_imputed=0, previous_cancellations_imputed=0, previous_bookings_not_canceled_imputed=0, booking_changes_imputed=0, days_in_waiting_list_imputed=0, required_car_parking_spaces_imputed=0, total_of_special_requests_imputed=0, hotel_indexed=1.0, arrival_date_month_indexed=0.0, meal_indexed=0.0, market_segment_indexed=3.0, distribution_channel_indexed=1.0, reserved_room_type_indexed=3.0, assigned_room_type_indexed=3.0, deposit_type_indexed=0.0, customer_type_indexed=0.0, reservation_status_indexed=0.0, hotel_indexed_encoded=SparseVector(4, {1: 1.0}), arrival_date_month_indexed_encoded=SparseVector(14, {0: 1.0}), meal_indexed_encoded=SparseVector(7, {0: 1.0}), market_segment_indexed_encoded=SparseVector(10, {3: 1.0}), distribution_channel_indexed_encoded=SparseVector(7, {1: 1.0}), reserved_room_type_indexed_encoded=SparseVector(12, {3: 1.0}), assigned_room_type_indexed_encoded=SparseVector(13, {3: 1.0}), deposit_type_indexed_encoded=SparseVector(5, {0: 1.0}), customer_type_indexed_encoded=SparseVector(6, {0: 1.0}), reservation_status_indexed_encoded=SparseVector(5, {0: 1.0}), features_raw=SparseVector(98, {1: 2015.0, 2: 32.0, 3: 4.0, 5: 1.0, 6: 1.0, 16: 1.0, 19: 1.0, 33: 1.0, 43: 1.0, 51: 1.0, 60: 1.0, 72: 1.0, 82: 1.0, 87: 1.0, 93: 1.0}), features=SparseVector(68, {1: 2015.0, 2: 32.0, 3: 4.0, 5: 1.0, 6: 1.0, 16: 1.0, 17: 1.0, 28: 1.0, 34: 1.0, 39: 1.0, 45: 1.0, 52: 1.0, 59: 1.0, 62: 1.0}), rawPrediction=DenseVector([1.3259, -1.3259]), probability=DenseVector([0.9341, 0.0659]), prediction=0.0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predição original com o melhor modelo\n",
    "predicao_teste = best_model.transform(test_df)\n",
    "predicao_teste.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5cce1f0e-6e78-44d7-8de2-a45dee817e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 1.00\n"
     ]
    }
   ],
   "source": [
    "dt_hyper_eval = BinaryClassificationEvaluator(rawPredictionCol=\"probability\", labelCol=\"label\")\n",
    "dt_hyper_AUC  = dt_hyper_eval.evaluate(predicao_teste)\n",
    "print(\"AUC = %.2f\" % dt_hyper_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e737a44c-9dda-4db0-9205-d5f23250ace6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+-----+\n",
      "|prediction_label|  0.0|  1.0|\n",
      "+----------------+-----+-----+\n",
      "|             0.0|22471|    0|\n",
      "|             1.0|    0|13325|\n",
      "+----------------+-----+-----+\n",
      "\n",
      "Accuracy = 1.00\n",
      "Sensitivity = 1.00\n",
      "Specificity = 1.00\n",
      "Precision = 1.00\n"
     ]
    }
   ],
   "source": [
    "#preds_and_labels = predictionAndLabels.select(['predictions','d']).withColumn('label', F.col('d').cast(FloatType())).orderBy('prediction')\n",
    "#important: need to cast to float type, and order by prediction, else it won't work\n",
    "\n",
    "#select only prediction and label columns\n",
    "# preds_and_labels = predicao_teste.select(['prediction','label']).withColumn('label', col('label').cast(FloatType())).orderBy('prediction')\n",
    "# metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "# matriz_array = metrics.confusionMatrix().toArray()\n",
    "# print(matriz_array)\n",
    "\n",
    "# Criação da Matriz de confução\n",
    "cm_dt_result = predicao_teste.crosstab(\"prediction\", \"label\").orderBy(col('prediction_label').asc())\n",
    "cm_dt_result.show()\n",
    "\n",
    "#Convertir para listas\n",
    "novas_colunas = [f\"{c}_raname\".replace(\".\",\",\") for c in cm_dt_result.columns]\n",
    "cm_dt_result_t = cm_dt_result.toDF(*novas_colunas)\n",
    "cm_dt_result_t\n",
    "\n",
    "vals = [linha[:][:] for linha in cm_dt_result_t.select(*novas_colunas[1:]).collect()] \n",
    "# cm_dt_result_t.select(col(novas_colunas[1])).collect()\n",
    "\n",
    "# [(22471, 0), (0, 13325)]\n",
    "#calculate accuracy, sensitivity, specificity and precision\n",
    "TP = vals[1][1]\n",
    "FP = vals[1][0]\n",
    "TN = vals[0][0]\n",
    "FN = vals[0][1]\n",
    "\n",
    "Accuracy = (TP+TN)/(TP+FP+TN+FN)\n",
    "Sensitivity = TP/(TP+FN)\n",
    "Specificity = TN/(TN+FP)\n",
    "Precision = TP/(TP+FP)\n",
    "\n",
    "print (\"Accuracy = %0.2f\" %Accuracy )\n",
    "print (\"Sensitivity = %0.2f\" %Sensitivity )\n",
    "print (\"Specificity = %0.2f\" %Specificity )\n",
    "print (\"Precision = %0.2f\" %Precision )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "87110810-ee5f-4371-ba4b-45b777f324a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teste_prob = predicao_teste.select('probability', 'prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "33ed922d-6680-4eed-9210-8238a8d0ff2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(probability=DenseVector([0.9341, 0.0659]), prediction=0.0)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_teste_prob.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b5117f94-a62a-4ce0-872a-aabddd8914e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o238275.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 14520.0 failed 1 times, most recent failure: Lost task 0.0 in stage 14520.0 (TID 72180) (af9d95f152c7 executor driver): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype). This happens when an unsupported/unregistered class is being unpickled that requires construction arguments. Fix it by registering a custom IObjectConstructor for this class.\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:759)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:199)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$2(BatchEvalPythonExec.scala:67)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype). This happens when an unsupported/unregistered class is being unpickled that requires construction arguments. Fix it by registering a custom IObjectConstructor for this class.\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:759)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:199)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$2(BatchEvalPythonExec.scala:67)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m df_com_segundo_valor \u001b[38;5;241m=\u001b[39m df_teste_prob\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegundo_valor\u001b[39m\u001b[38;5;124m\"\u001b[39m, extract_second_value_udf(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprobability\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Imprimindo o dataframe\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf_com_segundo_valor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    954\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    955\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    956\u001b[0m     )\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o238275.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 14520.0 failed 1 times, most recent failure: Lost task 0.0 in stage 14520.0 (TID 72180) (af9d95f152c7 executor driver): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype). This happens when an unsupported/unregistered class is being unpickled that requires construction arguments. Fix it by registering a custom IObjectConstructor for this class.\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:759)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:199)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$2(BatchEvalPythonExec.scala:67)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype). This happens when an unsupported/unregistered class is being unpickled that requires construction arguments. Fix it by registering a custom IObjectConstructor for this class.\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:759)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:199)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$2(BatchEvalPythonExec.scala:67)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "def extract_second_value(probability_struct):\n",
    "  # Assuming \"values\" is the field containing the probabilities\n",
    "  return probability_struct.values[1]\n",
    "\n",
    "# Register the UDF\n",
    "extract_second_value_udf = spark.udf.register(\"extract_second_value\", extract_second_value, DoubleType())\n",
    "\n",
    "df_com_segundo_valor = df_teste_prob.withColumn(\"segundo_valor\", extract_second_value_udf(col(\"probability\")))\n",
    "\n",
    "# Imprimindo o dataframe\n",
    "print(df_com_segundo_valor.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19744963-8dd3-46b8-94a7-f50142850f09",
   "metadata": {},
   "source": [
    "### Predição com diferentes tresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "56ee3e09-6b04-4b5f-8db2-48f9e4d95779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: [0.3, 0.7]\n"
     ]
    }
   ],
   "source": [
    "# FAzendo predição com o modelo com trheshould modificado\n",
    "treshold_value = [0.3,0.7]\n",
    "best_model.stages[-1].setThresholds(treshold_value)\n",
    "print(f\"Threshold: {best_model.stages[-1].getThresholds()}\")\n",
    "# Predição original com o melhor modelo\n",
    "predicao_teste_treshold = best_model.transform(test_df)\n",
    "\n",
    "# predicao_teste_thesold = best_model_treshold.transform(test_df)\n",
    "# predicao_teste_treshold.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0d29b505-d10e-4792-b205-3c4ea79facaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 1.00\n"
     ]
    }
   ],
   "source": [
    "dt_hyper_eval = BinaryClassificationEvaluator(rawPredictionCol=\"probability\", labelCol=\"label\")\n",
    "dt_hyper_AUC  = dt_hyper_eval.evaluate(predicao_teste_treshold)\n",
    "print(\"AUC = %.2f\" % dt_hyper_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0ddc7456-acff-4919-adcd-aab5626b37d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+-----+\n",
      "|prediction_label|  0.0|  1.0|\n",
      "+----------------+-----+-----+\n",
      "|             0.0|22471|    0|\n",
      "|             1.0|    0|13325|\n",
      "+----------------+-----+-----+\n",
      "\n",
      "Accuracy = 1.00\n",
      "Sensitivity = 1.00\n",
      "Specificity = 1.00\n",
      "Precision = 1.00\n"
     ]
    }
   ],
   "source": [
    "# Criação da Matriz de confução\n",
    "cm_dt_result = predicao_teste_treshold.crosstab(\"prediction\", \"label\").orderBy(col('prediction_label').asc())\n",
    "cm_dt_result.show()\n",
    "\n",
    "#Convertir para listas\n",
    "novas_colunas = [f\"{c}_raname\".replace(\".\",\",\") for c in cm_dt_result.columns]\n",
    "cm_dt_result_t = cm_dt_result.toDF(*novas_colunas)\n",
    "cm_dt_result_t\n",
    "\n",
    "vals = [linha[:][:] for linha in cm_dt_result_t.select(*novas_colunas[1:]).collect()] \n",
    "# cm_dt_result_t.select(col(novas_colunas[1])).collect()\n",
    "\n",
    "# [(22471, 0), (0, 13325)]\n",
    "#calculate accuracy, sensitivity, specificity and precision\n",
    "TP = vals[1][1]\n",
    "FP = vals[1][0]\n",
    "TN = vals[0][0]\n",
    "FN = vals[0][1]\n",
    "\n",
    "Accuracy = (TP+TN)/(TP+FP+TN+FN)\n",
    "Sensitivity = TP/(TP+FN)\n",
    "Specificity = TN/(TN+FP)\n",
    "Precision = TP/(TP+FP)\n",
    "\n",
    "print (\"Accuracy = %0.2f\" %Accuracy )\n",
    "print (\"Sensitivity = %0.2f\" %Sensitivity )\n",
    "print (\"Specificity = %0.2f\" %Specificity )\n",
    "print (\"Precision = %0.2f\" %Precision )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d482c948-d137-4645-9a43-c7269254804a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar a predição\n",
    "caminho_parquet = 'work/data/Output/predictions_threshold.parquet'\n",
    "predicao_teste_treshold.write.format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .option(\"path\", caminho_parquet) \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0506ea-3ac0-4b90-ba3d-4203279d8a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
