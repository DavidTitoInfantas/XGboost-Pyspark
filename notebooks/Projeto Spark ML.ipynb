{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f2b6492-596a-492d-8671-8623608a6818",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Install library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657f848d-f183-4bc4-8ff6-ef9c908362e6",
   "metadata": {},
   "source": [
    "#### Projeto localizado no site \n",
    "https://archive.ics.uci.edu/dataset/45/heart+disease\n",
    "\n",
    "\n",
    "#### Introdução pyspark\n",
    "https://blog.devgenius.io/beginners-introduction-to-big-data-in-pyspark-9931d919521c\n",
    "\n",
    "#### Transformações em pyspark feature engineering\n",
    "https://medium.com/@nutanbhogendrasharma/feature-transformer-vectorassembler-in-pyspark-ml-feature-part-3-b3c2c3c93ee9\n",
    "\n",
    "#### manipulação e combinação de tatasets com pyspark (joins)\n",
    "https://datalivre.medium.com/joins-em-pyspark-3c1d2773eeb1\n",
    "\n",
    "##### Descrição dos campos:\n",
    "Only 14 attributes used:\n",
    "      1. #3  (age\n",
    "          3 age: age in years \n",
    "      2. #4  (sex)\n",
    "          4 sex: sex (1 = male; 0 = female)  \n",
    "      3. #9  (cp) \n",
    "          9 cp: chest pain type\n",
    "        -- Value 1: typical angina\n",
    "        -- Value 2: atypical angina\n",
    "        -- Value 3: non-anginal pain\n",
    "        -- Value 4: asymptomatic   \n",
    "      4. #10 (t\n",
    "          10 trestbps: resting blood pressure (in mm Hg on admission to the hospital)stbps)  \n",
    "      5. #1\n",
    "          12 chol: serum cholestoral in mg/dlol)      \n",
    "      6.\n",
    "          16 fbs: (fasting blood sugar > 120 mg/dl)  (1 = true; 0 = false)bs)       \n",
    "      7. #19 (\n",
    "          19 restecg: resting electrocardiographic results\n",
    "        -- Value 0: normal\n",
    "        -- Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
    "        -- Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteriarestecg)   \n",
    "      8.\n",
    "          32 thalach: maximum heart rate achieved32 (thalach)   \n",
    "      9\n",
    "          38 exang: exercise induced angina (1 = yes; 0 = no)#38 (ang)     \n",
    "      10\n",
    "          40 oldpeak = ST depression induced by exercise relative to rest #40 ldpeak)   \n",
    "      1\n",
    "          41 slope: the slope of the peak exercise ST segment\n",
    "        -- Value 1: upsloping\n",
    "        -- Value 2: flat\n",
    "        -- Value 3: downsloping. 1 (slope)     \n",
    "\n",
    "          44 ca: number of major vessels (0-3) colored by flourosopy  12.44 (ca)        \n",
    "          51 thal: 3 = normal; 6 = fixed defect; 7 = reversable defect   13#51 (thal)      \n",
    "      14. #58 (num)       (\n",
    "          58 num: diagnosis of heart disease (angiographic disease status)\n",
    "        -- Value 0: < 50% diameter narrowing\n",
    "        -- Value 1: > 50% diameter narrowing\n",
    "        (in any major vessel: attributes 59 through 68 are vessels)the predicted attribute)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da3d0d59-00d9-4059-bccf-25a483a3f272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.23.* in /opt/conda/lib/python3.11/site-packages (1.23.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.23.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a41228e7-b78b-48e7-a8d9-227d74de1419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdc472a3-e54d-4244-ae0a-33e2b3015754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.23.5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7091157-581b-423c-a38f-5aaf69984eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pyspark\n",
    "!pip install -q handyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cc7af50-483c-467c-a540-50b395b7f15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "import urllib.request\n",
    "from pyspark.sql.functions import col, count, isnan, when\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79cfa5fc-a444-4dd8-82ee-40426d026c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Session\n",
    "conf = SparkConf() \\\n",
    "    .set(\"spark.executor.instances\", \"2\") \\\n",
    "    .set(\"spark.executor.memory\", \"2g\") \\\n",
    "    .set(\"spark.driver.memory\", \"2g\") \\\n",
    "    .setAppName(\"MeuAPP\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .config(conf=conf) \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Obter o contexto Spark da sessão\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# # Parar a sessão Spark quando não for mais necessária\n",
    "# spark.stop()\n",
    "\n",
    "# # Parar o contexto Spark quando não for mais necessário\n",
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c394daea-0e05-4a28-891f-edc03a673079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session Spark:\n",
      " - ID da aplicação: local-1707790600046\n",
      " - Nome da aplicação: MeuAPP\n",
      "\n",
      "Contexto Spark:\n",
      " - Versão do Spark: 3.5.0\n",
      " - Modo de execução: local[*]\n",
      "\n",
      "The Python version of Spark Context in the PySpark shell is 3.11\n"
     ]
    }
   ],
   "source": [
    "# Imprimir informações sobre a sessão Spark\n",
    "print(\"Session Spark:\")\n",
    "print(\" - ID da aplicação:\", spark.sparkContext.applicationId)\n",
    "print(\" - Nome da aplicação:\", spark.sparkContext.appName)\n",
    "\n",
    "# Imprimir informações sobre o contexto Spark\n",
    "print(\"\\nContexto Spark:\")\n",
    "print(\" - Versão do Spark:\", sc.version)\n",
    "print(\" - Modo de execução:\", sc.master)\n",
    "\n",
    "# Print the Python version of SparkContext\n",
    "print(\"\\nThe Python version of Spark Context in the PySpark shell is\", sc.pythonVer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8f8b10d-d213-40c5-80a9-cf5005c2e693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://6dd310dc92f3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MeuAPP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f20af246810>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "733fc2de-9cf7-4d45-869b-69a9eede7218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esquema dos Dados:\n",
      "root\n",
      " |-- age: double (nullable = true)\n",
      " |-- sex: double (nullable = true)\n",
      " |-- cp: double (nullable = true)\n",
      " |-- trestbps: double (nullable = true)\n",
      " |-- chol: double (nullable = true)\n",
      " |-- fbs: double (nullable = true)\n",
      " |-- restecg: double (nullable = true)\n",
      " |-- thalach: double (nullable = true)\n",
      " |-- exang: double (nullable = true)\n",
      " |-- oldpeak: double (nullable = true)\n",
      " |-- slope: double (nullable = true)\n",
      " |-- ca: string (nullable = true)\n",
      " |-- thal: string (nullable = true)\n",
      " |-- num: integer (nullable = true)\n",
      "\n",
      "Shape of the dataset:  (303, 14)\n",
      "\n",
      "Primeiras 5 linhas dos Dados:\n",
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+---+\n",
      "| age|sex| cp|trestbps| chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|num|\n",
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+---+\n",
      "|63.0|1.0|1.0|   145.0|233.0|1.0|    2.0|  150.0|  0.0|    2.3|  3.0|0.0| 6.0|  0|\n",
      "|67.0|1.0|4.0|   160.0|286.0|0.0|    2.0|  108.0|  1.0|    1.5|  2.0|3.0| 3.0|  2|\n",
      "|67.0|1.0|4.0|   120.0|229.0|0.0|    2.0|  129.0|  1.0|    2.6|  2.0|2.0| 7.0|  1|\n",
      "|37.0|1.0|3.0|   130.0|250.0|0.0|    0.0|  187.0|  0.0|    3.5|  3.0|0.0| 3.0|  0|\n",
      "|41.0|0.0|2.0|   130.0|204.0|0.0|    2.0|  172.0|  0.0|    1.4|  1.0|0.0| 3.0|  0|\n",
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Link para o conjunto de dados Iris\n",
    "link_dados = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
    "local_path = \"./data/raw/heard_disease_raw_data.csv\"  # Caminho local onde o arquivo será salvo\n",
    "\n",
    "# Baixar o arquivo CSV do URL\n",
    "urllib.request.urlretrieve(link_dados, local_path)\n",
    "\n",
    "# Nomes colunas\n",
    "nomes_colunas=[\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \n",
    "                \"restecg\", \"thalach\", \"exang\", \"oldpeak\",\n",
    "                \"slope\",\"ca\",\"thal\",\"num\"\n",
    "              ]\n",
    "\n",
    "# Carregar os dados como um DataFrame Spark\n",
    "df = spark.read.csv(local_path, header=None, inferSchema=True)\n",
    "\n",
    "# Atribuir nomes às colunas\n",
    "df = df.toDF(*nomes_colunas)\n",
    "\n",
    "# Exibir o esquema dos dados\n",
    "print(\"Esquema dos Dados:\")\n",
    "df.printSchema()\n",
    "\n",
    "# Exibir a quantidade de colunas e linhas\n",
    "print(\"Shape of the dataset: \", (df.count(), len(df.columns)))\n",
    "\n",
    "# Exibir as primeiras 5 linhas dos dados\n",
    "print(\"\\nPrimeiras 5 linhas dos Dados:\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f19ceea-0e2a-482a-9fba-78a0435b8013",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_describe = df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bdc0c74-27b4-47d1-999d-43c84bdb7708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|summary|              age|                sex|                cp|          trestbps|              chol|               fbs|           restecg|           thalach|              exang|           oldpeak|             slope|                ca|              thal|               num|\n",
      "+-------+-----------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|  count|              303|                303|               303|               303|               303|               303|               303|               303|                303|               303|               303|               303|               303|               303|\n",
      "|   mean|54.43894389438944| 0.6798679867986799|3.1584158415841586|131.68976897689768|246.69306930693068|0.1485148514851485|0.9900990099009901| 149.6072607260726|0.32673267326732675|1.0396039603960396|1.6006600660066006|0.6722408026755853|  4.73421926910299|0.9372937293729373|\n",
      "| stddev| 9.03866244244675|0.46729882777012977|0.9601256119600138| 17.59974772958769|51.776917542637065|0.3561978749279763|0.9949712915251783|22.875003276980383|0.46979446452231644| 1.161075022068635|0.6162261453459619| 0.937438317724216|1.9397057693786433|1.2285356879701044|\n",
      "|    min|             29.0|                0.0|               1.0|              94.0|             126.0|               0.0|               0.0|              71.0|                0.0|               0.0|               1.0|               0.0|               3.0|                 0|\n",
      "|    max|             77.0|                1.0|               4.0|             200.0|             564.0|               1.0|               2.0|             202.0|                1.0|               6.2|               3.0|                 ?|                 ?|                 4|\n",
      "+-------+-----------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_describe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae6db58-7640-43e1-843e-941301faed10",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9d18d87-5005-4680-91b4-9d3f3b5dd199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|thal|count|\n",
      "+----+-----+\n",
      "| 6.0|   18|\n",
      "| 7.0|  117|\n",
      "|   ?|    2|\n",
      "| 3.0|  166|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('thal').agg(count(\"*\").alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11864e71-b703-4506-bab1-042a44aaade3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+---+\n",
      "|age|sex| cp|trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|num|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+---+\n",
      "|  0|  0|  0|       0|   0|  0|      0|      0|    0|      0|    0|  0|   0|  0|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_df = df.select([count(when(\n",
    "                                col(c).contains('None') | \\\n",
    "                                col(c).contains('NULL') | \\\n",
    "                                (col(c) == '') | \\\n",
    "                                col(c).isNull() | \\\n",
    "                                isnan(col(c)), c ))\n",
    "           .alias(c)\n",
    "           for c in df.columns])\n",
    "\n",
    "null_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa38bc88-6840-4c09-bb74-1722275737b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+---+\n",
      "| age|sex| cp|trestbps| chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|num|\n",
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+---+\n",
      "|50.0|0.0|3.0|   120.0|219.0|0.0|    0.0|  158.0|  0.0|    1.6|  2.0|0.0| 3.0|  0|\n",
      "|50.0|1.0|4.0|   150.0|243.0|0.0|    2.0|  128.0|  0.0|    2.6|  2.0|0.0| 7.0|  4|\n",
      "|50.0|1.0|3.0|   140.0|233.0|0.0|    0.0|  163.0|  0.0|    0.6|  2.0|1.0| 7.0|  1|\n",
      "|50.0|1.0|3.0|   129.0|196.0|0.0|    0.0|  163.0|  0.0|    0.0|  1.0|0.0| 3.0|  0|\n",
      "|50.0|0.0|2.0|   120.0|244.0|0.0|    0.0|  162.0|  0.0|    1.1|  1.0|0.0| 3.0|  0|\n",
      "|50.0|0.0|4.0|   110.0|254.0|0.0|    2.0|  159.0|  0.0|    0.0|  1.0|0.0| 3.0|  0|\n",
      "|50.0|1.0|4.0|   144.0|200.0|0.0|    2.0|  126.0|  1.0|    0.9|  2.0|0.0| 7.0|  3|\n",
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"*\").filter(col('age') == 50).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14b8b09-4136-413d-ad92-ab23ef93f2a6",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dba72c3d-05ef-42d1-9123-2abe9ed06a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################################\n",
    "# Trabalhando com RDD \n",
    "##########################################\n",
    "\n",
    "# Verification of the actuality dataset\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24053e86-0189-4073-8df6-1a6341972080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file type of fileRDD is <class 'pyspark.rdd.RDD'>\n",
      "Number of partitions in fileRDD is 2\n"
     ]
    }
   ],
   "source": [
    "# Create a fileRDD from filepath\n",
    "fileRDD = sc.textFile(local_path)\n",
    "# Check the type of fileRDD\n",
    "print(\"The file type of fileRDD is\", type(fileRDD))\n",
    "# Check the number of partitions in fileRDD\n",
    "print(\"Number of partitions in fileRDD is\", fileRDD.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bbb52b6-08b6-4282-838d-86029baf3e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions after to put a specific partitions number  5\n"
     ]
    }
   ],
   "source": [
    "# Create a fileRDD_part from file_path with 5 partitions\n",
    "fileRDD_part = sc.textFile(local_path, minPartitions = 5)\n",
    "print(\"Number of partitions after to put a specific partitions number \",fileRDD_part.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "358068e5-6ff8-479e-a2bf-4ea580a2da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbRDD = sc.parallelize(range(1,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5959790-442f-40be-8a95-662cd2fcc4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbRDD.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f2d3e02-fd0c-4737-878b-dd1b804cc013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 8, 27, 64, 125, 216, 343, 512, 729, 1000]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cubedRDD = numbRDD.map(lambda x: x**3)\n",
    "cubedRDD.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6345eec-3e73-4068-8e8b-cb040b392a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 8, 27, 64, 125, 216, 343, 512, 729, 1000]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect the results\n",
    "numbers_all = cubedRDD.collect()\n",
    "numbers_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6647044-ff7e-40e5-984a-5520fb596ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "8\n",
      "27\n",
      "64\n",
      "125\n",
      "216\n",
      "343\n",
      "512\n",
      "729\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "#Practice SC functions do contexto\n",
    "#Create map() transformation to cube numbers\n",
    "numbRDD = sc.parallelize(range(1,11))\n",
    "cubedRDD = numbRDD.map(lambda x: x**3)\n",
    "# OBS: The result can´t be show with function show(), in this case we need use take()\n",
    "# cubedRDD.take(5)\n",
    "\n",
    "# Collect the results\n",
    "numbers_all = cubedRDD.collect()\n",
    "\n",
    "# Print the numbers from numbers_all\n",
    "for numb in numbers_all:\n",
    " print(numb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbe67eee-00a1-4b60-8cd1-620872ba9319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of lines with the keyword Spark is 4\n",
      "51.0,1.0,3.0,100.0,222.0,0.0,0.0,143.0,1.0,1.2,2.0,0.0,3.0,0\n",
      "58.0,0.0,4.0,100.0,248.0,0.0,2.0,122.0,0.0,1.0,2.0,0.0,3.0,0\n",
      "67.0,1.0,4.0,100.0,299.0,0.0,2.0,125.0,1.0,0.9,2.0,2.0,3.0,3\n",
      "58.0,1.0,4.0,100.0,234.0,0.0,0.0,156.0,0.0,0.1,1.0,1.0,7.0,2\n"
     ]
    }
   ],
   "source": [
    "# Filter the fileRDD to select lines with Spark keyword\n",
    "fileRDD_filter = fileRDD.filter(lambda line: '100' in line)\n",
    "\n",
    "# How many lines are there in fileRDD?\n",
    "print(\"The total number of lines with the keyword Spark is\", fileRDD_filter.count())\n",
    "\n",
    "# Print the first four lines of fileRDD\n",
    "for line in fileRDD_filter.take(6): \n",
    "  print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7337353-00e9-4d3a-be0f-cab8a586820d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 1 has 2 Counts\n",
      "Key 3 has 10 Counts\n",
      "Key 4 has 5 Counts\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# Trabalhando com RDD Pairs transformations\n",
    "##########################\n",
    "# Create PairRDD Rdd with key value pairs\n",
    "Rdd = sc.parallelize([(1,2),(3,4),(3,6),(4,5)])\n",
    "\n",
    "# Apply reduceByKey() operation on Rdd\n",
    "Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Iterate over the result and print the output\n",
    "for num in Rdd_Reduced.collect(): \n",
    "  print(\"Key {} has {} Counts\".format(num[0], num[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6577bff1-9193-4e26-9c53-4fc8ab4b19c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 4 has 5 Counts\n",
      "Key 3 has 10 Counts\n",
      "Key 1 has 2 Counts\n"
     ]
    }
   ],
   "source": [
    "# Sort the reduced RDD with the key by descending order\n",
    "Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)\n",
    "\n",
    "# Iterate over the result and retrieve all the elements of the RDD\n",
    "for num in Rdd_Reduced_Sort.collect():\n",
    "  print(\"Key {} has {} Counts\".format(num[0], num[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16435a2b-feeb-46bd-96b3-494e01e8100e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of total is <class 'collections.defaultdict'>\n",
      "key 1 has 1 counts\n",
      "key 3 has 2 counts\n",
      "key 4 has 1 counts\n"
     ]
    }
   ],
   "source": [
    "# Count the unique keys\n",
    "total = Rdd.countByKey()\n",
    "\n",
    "# What is the type of total?\n",
    "print(\"The type of total is\", type(total))\n",
    "\n",
    "# Iterate over the total and print the output\n",
    "for k, v in total.items(): \n",
    "  print(\"key\", k, \"has\", v, \"counts\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a22d21d2-48df-4f52-b774-b46b350674ff",
   "metadata": {},
   "source": [
    "Operating on Dataframes in PySpark\n",
    "Dataframe operators in PySpark\n",
    "\n",
    "Dataframe Transformations -\n",
    "    select() — subsets the columns in a dataframe\n",
    "    \n",
    "    filter() — selection on the basis of a condition\n",
    "    \n",
    "    groupby() — group dataframe using the specified columns\n",
    "    \n",
    "    orderby() — return dataframe sorted by one or more column\n",
    "    \n",
    "    dropDuplicates() — removes duplicate rows\n",
    "    \n",
    "    withColumnRenamed() — renames a column\n",
    "\n",
    "    distinct()\n",
    "\n",
    "    when(condicao,1)\n",
    "\n",
    "    withColumn(\"colum_name\", when(condicao,1).otherwise(0))\n",
    "\n",
    "Dataframe Actions -\n",
    "    head() — returns the first n number of rows as Row objects\n",
    "    \n",
    "    printSchema() — return datatypes of columns in dataframe\n",
    "    \n",
    "    columns() — prints columns of dataframe\n",
    "    \n",
    "    show() — prints first 20 rows in dataframe\n",
    "    \n",
    "    count() — counts number of rows in the dataframe\n",
    "    \n",
    "    describe() — summary statistics of numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c64218bd-e113-44c1-a71c-0c3879feb7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "|sex|trestbps|slope|\n",
      "+---+--------+-----+\n",
      "|1.0|   145.0|  3.0|\n",
      "|1.0|   160.0|  2.0|\n",
      "|1.0|   120.0|  2.0|\n",
      "|1.0|   130.0|  3.0|\n",
      "|0.0|   130.0|  1.0|\n",
      "+---+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######################\n",
    "# Managment with dataframe\n",
    "#######################\n",
    "\n",
    "# Select _c0,_c1,_c2 of birth columns\n",
    "df_sub = df.select('sex', 'trestbps', 'slope')\n",
    "df_sub.show(5)\n",
    "df_sub.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fcd0251a-4561-429e-8dd6-bdcced077f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 303 rows before removing duplicates, and 125 rows after removing duplicates\n"
     ]
    }
   ],
   "source": [
    "df_sub_nodup = df_sub.dropDuplicates()\n",
    "\n",
    "# Count the number of rows\n",
    "print(\"There were {} rows before removing duplicates, and {} rows after removing duplicates\"\\\n",
    "      .format(df_sub.count(), df_sub_nodup.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06be56f7-7eb6-4f60-9984-9b8ebf8abaed",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'people_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Filter people_df to select Nunavut \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m people_df_Nunavut \u001b[38;5;241m=\u001b[39m \u001b[43mpeople_df\u001b[49m\u001b[38;5;241m.\u001b[39mfilter(people_df\u001b[38;5;241m.\u001b[39m_c7 \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNunavut\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Filter people_df to select Northwest Territories\u001b[39;00m\n\u001b[1;32m      5\u001b[0m people_df_nt \u001b[38;5;241m=\u001b[39m people_df\u001b[38;5;241m.\u001b[39mfilter(people_df\u001b[38;5;241m.\u001b[39m_c7 \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNorthwest Territories\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'people_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Filter people_df to select Nunavut \n",
    "people_df_Nunavut = people_df.filter(people_df._c7 == \"Nunavut\")\n",
    "\n",
    "# Filter people_df to select Northwest Territories\n",
    "people_df_nt = people_df.filter(people_df._c7 == \"Northwest Territories\")\n",
    "\n",
    "# Count the number of rows \n",
    "print(\"There are {} rows in the people_df_female DataFrame and {} rows in the people_df_male DataFrame\"\\\n",
    "      .format(people_df_Nunavut.count(), people_df_nt.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a840604-6000-40c7-80ee-155ddadcbda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 206 males e 97 females in the dataframe\n"
     ]
    }
   ],
   "source": [
    "# 1=Male, 0=Female\n",
    "df_sub_male = df_sub.filter(df_sub.sex == 1)\n",
    "df_sub_female = df_sub.filter(df_sub.sex == 0)\n",
    "\n",
    "print(f\"There are {df_sub_male.count()} males e {df_sub_female.count()} females in the dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4bcf9efa-79ce-4aa4-ad07-69f7465d42ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+---+\n",
      "| age|sex| cp|trestbps| chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|num|\n",
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+---+\n",
      "|63.0|1.0|1.0|   145.0|233.0|1.0|    2.0|  150.0|  0.0|    2.3|  3.0|0.0| 6.0|  0|\n",
      "|67.0|1.0|4.0|   160.0|286.0|0.0|    2.0|  108.0|  1.0|    1.5|  2.0|3.0| 3.0|  2|\n",
      "|67.0|1.0|4.0|   120.0|229.0|0.0|    2.0|  129.0|  1.0|    2.6|  2.0|2.0| 7.0|  1|\n",
      "|37.0|1.0|3.0|   130.0|250.0|0.0|    0.0|  187.0|  0.0|    3.5|  3.0|0.0| 3.0|  0|\n",
      "|41.0|0.0|2.0|   130.0|204.0|0.0|    2.0|  172.0|  0.0|    1.4|  1.0|0.0| 3.0|  0|\n",
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e668252-de1f-44f9-84e9-7c5bf5d817bb",
   "metadata": {},
   "source": [
    "##############################\n",
    "# Executing SQL Querys \n",
    "##############################\n",
    "\n",
    "SparkSession sql() method executes SQL query\n",
    "SQL cannot be run against a dataframe so we have to create a temporary view using createOrReplaceTempView() command and run SQL queries and it will return a dataframe\n",
    "SYNTAX — df.createOrReplaceTempView(“test”)\n",
    ">>test_df = spark.sql(queryinquotes from test)\n",
    "\n",
    ">>test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7572705e-45d1-4af7-b971-e7aa425f993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary table \"df_base\"\n",
    "df.createOrReplaceTempView(\"df_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ab2ff5f-ea23-4609-b203-2add0b902b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'sex',\n",
       " 'cp',\n",
       " 'trestbps',\n",
       " 'chol',\n",
       " 'fbs',\n",
       " 'restecg',\n",
       " 'thalach',\n",
       " 'exang',\n",
       " 'oldpeak',\n",
       " 'slope',\n",
       " 'ca',\n",
       " 'thal',\n",
       " 'num']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c24e4ac6-c2e8-4a08-98a6-fdaa476d0167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "| age|qtd|\n",
      "+----+---+\n",
      "|58.0| 19|\n",
      "|57.0| 17|\n",
      "|54.0| 16|\n",
      "|59.0| 14|\n",
      "|52.0| 13|\n",
      "+----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Contrução da query\n",
    "query = '''SELECT age, \n",
    "                count(Age) AS qtd\n",
    "            FROM df_base\n",
    "            GROUP BY age\n",
    "            Order BY qtd DESC\n",
    "            LIMIT 100'''\n",
    "\n",
    "df_age_agr = spark.sql(query)\n",
    "df_age_agr.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d46ee533-1431-4478-a371-8c2ed71eadb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|new_cp|new_ca|\n",
      "+------+------+\n",
      "|   1.0|   0.0|\n",
      "|   4.0|   3.0|\n",
      "|   4.0|   2.0|\n",
      "|   3.0|   0.0|\n",
      "|   2.0|   0.0|\n",
      "|   4.0|   1.0|\n",
      "|   4.0|   0.0|\n",
      "|   4.0|   0.0|\n",
      "|   3.0|   1.0|\n",
      "|   2.0|   0.0|\n",
      "+------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col('sex') == 1).select(col('cp').alias('new_cp'),col('ca').alias('new_ca')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30d72b37-3067-478d-bcd5-e045202cf862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| ca|count(1)|\n",
      "+---+--------+\n",
      "|0.0|     176|\n",
      "|1.0|      65|\n",
      "|2.0|      38|\n",
      "|3.0|      20|\n",
      "|  ?|       4|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analisando dados invalidao \"ca\"\n",
    "df.groupBy(\"ca\").agg(count(\"*\")).orderBy(\"ca\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80b905f3-d846-4b54-9438-2b1a9cdd18d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|thal|count(1)|\n",
      "+----+--------+\n",
      "| 3.0|     166|\n",
      "| 6.0|      18|\n",
      "| 7.0|     117|\n",
      "|   ?|       2|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analisando dados invalidao \"ca\"\n",
    "df.groupBy(\"thal\").agg(count(\"*\")).orderBy(\"thal\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30174d4f-b8bd-46e4-8588-187078d8f16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletando linhas com erro\n",
    "df_clean = df.filter(col(\"ca\")!=\"?\").filter(col(\"thal\")!='?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a27b71cd-ea33-411f-9218-9e361e03db9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a8e24edd-ca81-4aac-9853-19ee1214d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_format = df_clean.withColumn(\"ca\", col(\"ca\").cast(\"double\")) \\\n",
    "        .withColumn(\"thal\",col(\"thal\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ecc1633-7cc2-49b9-ba55-1ba6201224af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: double (nullable = true)\n",
      " |-- sex: double (nullable = true)\n",
      " |-- cp: double (nullable = true)\n",
      " |-- trestbps: double (nullable = true)\n",
      " |-- chol: double (nullable = true)\n",
      " |-- fbs: double (nullable = true)\n",
      " |-- restecg: double (nullable = true)\n",
      " |-- thalach: double (nullable = true)\n",
      " |-- exang: double (nullable = true)\n",
      " |-- oldpeak: double (nullable = true)\n",
      " |-- slope: double (nullable = true)\n",
      " |-- ca: double (nullable = true)\n",
      " |-- thal: double (nullable = true)\n",
      " |-- num: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean_format.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ada7282-43d8-4cfa-ae32-2e60135e2533",
   "metadata": {},
   "source": [
    "### Predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "c5c0f8a8-3cd1-49f8-af0c-6116e046a594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost==1.6.*\n",
      "  Downloading xgboost-1.6.2-py3-none-manylinux2014_x86_64.whl (255.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.9/255.9 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from xgboost==1.6.*) (1.23.5)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from xgboost==1.6.*) (1.11.3)\n",
      "Installing collected packages: xgboost\n",
      "  Attempting uninstall: xgboost\n",
      "    Found existing installation: xgboost 2.0.3\n",
      "    Uninstalling xgboost-2.0.3:\n",
      "      Successfully uninstalled xgboost-2.0.3\n",
      "Successfully installed xgboost-1.6.2\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost==1.6.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "4befc265-9ebb-4e90-92bb-79894899905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create model with XGBoost Livery\n",
    "# !pip install xgboost==1.6.8\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "# from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import rand, round\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "17714701-8695-46a9-b0e2-d6bcd927ab08",
   "metadata": {},
   "outputs": [],
   "source": [
    " xgb_classifier = SparkXGBClassifier(num_workers=2,\n",
    "                                           use_gpu=True,\n",
    "                                           max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "52411219-2772-489d-bef3-c86e40d0af05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_spark_df(X, y):\n",
    "#     return spark.createDataFrame(\n",
    "#         spark.sparkContext.parallelize(\n",
    "#             [(Vectors.dense(features), float(label)) for features, label in zip(X, y)]\n",
    "#         ),\n",
    "#         [\"features\", \"label\"],\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "a0f89739-c409-4e3e-80ea-84e4232ab22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+---+\n",
      "| age|sex| cp|trestbps| chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|num|\n",
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+---+\n",
      "|63.0|1.0|1.0|   145.0|233.0|1.0|    2.0|  150.0|  0.0|    2.3|  3.0|0.0| 6.0|  0|\n",
      "|67.0|1.0|4.0|   160.0|286.0|0.0|    2.0|  108.0|  1.0|    1.5|  2.0|3.0| 3.0|  2|\n",
      "|67.0|1.0|4.0|   120.0|229.0|0.0|    2.0|  129.0|  1.0|    2.6|  2.0|2.0| 7.0|  1|\n",
      "|37.0|1.0|3.0|   130.0|250.0|0.0|    0.0|  187.0|  0.0|    3.5|  3.0|0.0| 3.0|  0|\n",
      "|41.0|0.0|2.0|   130.0|204.0|0.0|    2.0|  172.0|  0.0|    1.4|  1.0|0.0| 3.0|  0|\n",
      "|56.0|1.0|2.0|   120.0|236.0|0.0|    0.0|  178.0|  0.0|    0.8|  1.0|0.0| 3.0|  0|\n",
      "|62.0|0.0|4.0|   140.0|268.0|0.0|    2.0|  160.0|  0.0|    3.6|  3.0|2.0| 3.0|  3|\n",
      "|57.0|0.0|4.0|   120.0|354.0|0.0|    0.0|  163.0|  1.0|    0.6|  1.0|0.0| 3.0|  0|\n",
      "|63.0|1.0|4.0|   130.0|254.0|0.0|    2.0|  147.0|  0.0|    1.4|  2.0|1.0| 7.0|  2|\n",
      "|53.0|1.0|4.0|   140.0|203.0|1.0|    2.0|  155.0|  1.0|    3.1|  3.0|0.0| 7.0|  1|\n",
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean_format.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "b7378a87-6f39-4bed-9080-f36e49e120b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "|num|Quantidade|\n",
      "+---+----------+\n",
      "|  1|        54|\n",
      "|  3|        35|\n",
      "|  4|        13|\n",
      "|  2|        35|\n",
      "|  0|       160|\n",
      "+---+----------+\n",
      "\n",
      "+---+----------+\n",
      "|num|Quantidade|\n",
      "+---+----------+\n",
      "|  1|        54|\n",
      "|  3|        35|\n",
      "|  4|        13|\n",
      "|  2|        35|\n",
      "|  0|       160|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # -- Value 0: < 50% diameter narrowing\n",
    " #    -- Value 1: > 50% diameter narrowing\n",
    "# df.select(\"num\").distinct().show()\n",
    "df_clean_format.groupBy(\"num\").agg(count(\"*\").alias(\"Quantidade\")).show()\n",
    "df_clean_format.groupBy(\"num\").agg(count(\"*\").alias(\"Quantidade\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "67ec461c-191b-44e2-9c24-173b1528c0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create the label\n",
    "# 1 if num > 0\n",
    "# 0 if num = 0\n",
    "condicao_1 = (df.num > 0)\n",
    "condicao_2 = (df.num == 0)\n",
    "df_with_label = df_clean_format.withColumn(\"num_label\",when(condicao_1, 1.0).otherwise(0.0)).drop(\"num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "876b4505-df98-4a86-93cb-a80cad46908f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+---------+\n",
      "| age|sex| cp|trestbps| chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|num_label|\n",
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+---------+\n",
      "|63.0|1.0|1.0|   145.0|233.0|1.0|    2.0|  150.0|  0.0|    2.3|  3.0|0.0| 6.0|      0.0|\n",
      "|67.0|1.0|4.0|   160.0|286.0|0.0|    2.0|  108.0|  1.0|    1.5|  2.0|3.0| 3.0|      1.0|\n",
      "|67.0|1.0|4.0|   120.0|229.0|0.0|    2.0|  129.0|  1.0|    2.6|  2.0|2.0| 7.0|      1.0|\n",
      "|37.0|1.0|3.0|   130.0|250.0|0.0|    0.0|  187.0|  0.0|    3.5|  3.0|0.0| 3.0|      0.0|\n",
      "|41.0|0.0|2.0|   130.0|204.0|0.0|    2.0|  172.0|  0.0|    1.4|  1.0|0.0| 3.0|      0.0|\n",
      "|56.0|1.0|2.0|   120.0|236.0|0.0|    0.0|  178.0|  0.0|    0.8|  1.0|0.0| 3.0|      0.0|\n",
      "|62.0|0.0|4.0|   140.0|268.0|0.0|    2.0|  160.0|  0.0|    3.6|  3.0|2.0| 3.0|      1.0|\n",
      "|57.0|0.0|4.0|   120.0|354.0|0.0|    0.0|  163.0|  1.0|    0.6|  1.0|0.0| 3.0|      0.0|\n",
      "|63.0|1.0|4.0|   130.0|254.0|0.0|    2.0|  147.0|  0.0|    1.4|  2.0|1.0| 7.0|      1.0|\n",
      "|53.0|1.0|4.0|   140.0|203.0|1.0|    2.0|  155.0|  1.0|    3.1|  3.0|0.0| 7.0|      1.0|\n",
      "|57.0|1.0|4.0|   140.0|192.0|0.0|    0.0|  148.0|  0.0|    0.4|  2.0|0.0| 6.0|      0.0|\n",
      "|56.0|0.0|2.0|   140.0|294.0|0.0|    2.0|  153.0|  0.0|    1.3|  2.0|0.0| 3.0|      0.0|\n",
      "|56.0|1.0|3.0|   130.0|256.0|1.0|    2.0|  142.0|  1.0|    0.6|  2.0|1.0| 6.0|      1.0|\n",
      "|44.0|1.0|2.0|   120.0|263.0|0.0|    0.0|  173.0|  0.0|    0.0|  1.0|0.0| 7.0|      0.0|\n",
      "|52.0|1.0|3.0|   172.0|199.0|1.0|    0.0|  162.0|  0.0|    0.5|  1.0|0.0| 7.0|      0.0|\n",
      "|57.0|1.0|3.0|   150.0|168.0|0.0|    0.0|  174.0|  0.0|    1.6|  1.0|0.0| 3.0|      0.0|\n",
      "|48.0|1.0|2.0|   110.0|229.0|0.0|    0.0|  168.0|  0.0|    1.0|  3.0|0.0| 7.0|      1.0|\n",
      "|54.0|1.0|4.0|   140.0|239.0|0.0|    0.0|  160.0|  0.0|    1.2|  1.0|0.0| 3.0|      0.0|\n",
      "|48.0|0.0|3.0|   130.0|275.0|0.0|    0.0|  139.0|  0.0|    0.2|  1.0|0.0| 3.0|      0.0|\n",
      "|49.0|1.0|2.0|   130.0|266.0|0.0|    0.0|  171.0|  0.0|    0.6|  1.0|0.0| 3.0|      0.0|\n",
      "+----+---+---+--------+-----+---+-------+-------+-----+-------+-----+---+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_label.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "d88d04be-2970-438e-88f9-6798bdb65223",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Particionamento em teste e treino\n",
    "random_state = 67\n",
    "df_train, df_test = df_with_label.randomSplit(weights=[0.8,0.2], seed=random_state)\n",
    "\n",
    "# df_label_train_X = df_train.drop(\"num_label\")\n",
    "# df_label_train_y = df_train.select(\"num_label\")\n",
    "\n",
    "# df_label_test_X = df_test.drop(\"num_label\")\n",
    "# df_label_test_y = df_test.select(\"num_label\")\n",
    "\n",
    "# print(f\"\"\"tamanho do dataset de treino features: {(df_label_train_X.count(),len(df_label_train_X.columns))}\n",
    "#         \\ntamanho do dataset de treino label: {(df_label_train_y.count(),len(df_label_train_y.columns))}\"\"\")\n",
    "# print(\"-\"*50)\n",
    "# print(f\"\"\"\\ntamanho do dataset de test features: {(df_label_test_X.count(),len(df_label_test_X.columns))}\n",
    "#         \\ntamanho do dataset de test label: {(df_label_test_y.count(),len(df_label_test_y.columns))}\"\"\")\n",
    "# # print(f\"\\n tamanho do dataset de teste features: {}, label: {}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "99b49906-2488-4842-87a7-4114402eab0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: double (nullable = true)\n",
      " |-- sex: double (nullable = true)\n",
      " |-- cp: double (nullable = true)\n",
      " |-- trestbps: double (nullable = true)\n",
      " |-- chol: double (nullable = true)\n",
      " |-- fbs: double (nullable = true)\n",
      " |-- restecg: double (nullable = true)\n",
      " |-- thalach: double (nullable = true)\n",
      " |-- exang: double (nullable = true)\n",
      " |-- oldpeak: double (nullable = true)\n",
      " |-- slope: double (nullable = true)\n",
      " |-- ca: double (nullable = true)\n",
      " |-- thal: double (nullable = true)\n",
      " |-- num_label: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d92171-d26d-4148-80ad-a39911a6e594",
   "metadata": {},
   "source": [
    "Diferença entre Dense vetor e Sparce Vetor na saida do transform\n",
    "\n",
    "- **DenseVector**: É usado quando a maioria dos elementos do vetor é diferente de zero. Ele armazena todos os elementos do vetor sequencialmente, independentemente de serem zero ou não.\n",
    "- **SparseVector**: É usado quando a maioria dos elementos do vetor é zero. Ele armazena apenas os elementos não nulos e suas posições no vetor, economizando espaço de armazenamento.\n",
    "\n",
    "Note. No seu caso, alguns vetores são densos (DenseVector) e outros são esparsos (SparseVector). Isso ocorre porque o VectorAssembler detecta automaticamente a densidade dos dados e escolhe a representação mais eficiente para cada vetor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "9d376e37-c755-4362-8a49-6f07b1687c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRanformando features num vetor denso\n",
    "def vetorization_df(data):\n",
    "    colunas_entrada = ['age','sex','cp','trestbps','chol','fbs','restecg',\n",
    "                       'thalach','exang','oldpeak','slope','ca','thal']\n",
    "    assembler = VectorAssembler(inputCols=colunas_entrada, outputCol=\"features\")\n",
    "    df_com_vetor_denso = assembler.transform(data)\n",
    "    return df_com_vetor_denso\n",
    "    # return df_com_vetor_denso.select(col(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "758b89ab-de26-4e0a-bd90-43b11604003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_vetor = vetorization_df(df_train).select(col(\"features\"),col(\"num_label\").alias(\"label\"))\n",
    "\n",
    "df_test_vetor = vetorization_df(df_test).select(col(\"features\"),col(\"num_label\").alias(\"label\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "cec56e45-aeb2-40ac-9e27-3088cd09cda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-14 18:26:47,902 WARNING SparkXGBClassifier: _validate_gpu_params You have enabled GPU in spark local mode. Please make sure your local node has at least 2 GPUs\n",
      "WARNING:SparkXGBClassifier:You have enabled GPU in spark local mode. Please make sure your local node has at least 2 GPUs\n",
      "2024-02-14 18:26:48,073 INFO XGBoost-PySpark: _fit Running xgboost-1.6.2 on 2 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'device': 'cpu', 'max_depth': 5, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Running xgboost-1.6.2 on 2 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'device': 'cpu', 'max_depth': 5, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-02-14 18:26:48,148 INFO SparkXGBClassifier: _skip_stage_level_scheduling Stage-level scheduling in xgboost requires spark standalone or local-cluster mode\n",
      "INFO:SparkXGBClassifier:Stage-level scheduling in xgboost requires spark standalone or local-cluster mode\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Could not recover from a failed barrier ResultStage. Most recent failure reason: Stage failed because barrier task ResultTask(1665, 0) finished unsuccessfully.\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1231, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 912, in read_udfs\n    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nModuleNotFoundError: No module named 'xgboost.spark'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:2216)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3042)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[267], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Treinamento do modelo\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m xgb_classifier_model \u001b[38;5;241m=\u001b[39m \u001b[43mxgb_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train_vetor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/xgboost/spark/core.py:1136\u001b[0m, in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/xgboost/spark/core.py:1122\u001b[0m, in \u001b[0;36m_run_job\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Could not recover from a failed barrier ResultStage. Most recent failure reason: Stage failed because barrier task ResultTask(1665, 0) finished unsuccessfully.\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1231, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 912, in read_udfs\n    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nModuleNotFoundError: No module named 'xgboost.spark'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:2216)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3042)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "#Treinamento do modelo\n",
    "xgb_classifier_model = xgb_classifier.fit(df_train_vetor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d3640afb-f04c-4f19-8eec-ca8d6c10204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df_test_vetor = xgb_classifier_model.transform(df_test_vetor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c369485d-345d-4917-ac45-a0d5ecde1102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+----------+--------------------+\n",
      "|            features|label|       rawPrediction|prediction|         probability|\n",
      "+--------------------+-----+--------------------+----------+--------------------+\n",
      "|[34.0,1.0,1.0,118...|  0.0|[6.14322900772094...|       0.0|[0.99785661697387...|\n",
      "|[35.0,1.0,4.0,126...|  1.0|[-3.9950373172760...|       1.0|[0.01807403564453...|\n",
      "|(13,[0,2,3,4,7,10...|  0.0|[8.07112503051757...|       0.0|[0.99968767166137...|\n",
      "|[37.0,1.0,3.0,130...|  0.0|[2.18687820434570...|       0.0|[0.89906495809555...|\n",
      "|(13,[0,2,3,4,7,10...|  0.0|[6.39008617401123...|       0.0|[0.99832469224929...|\n",
      "|[41.0,0.0,2.0,130...|  0.0|[7.96419095993042...|       0.0|[0.99965244531631...|\n",
      "|[41.0,1.0,2.0,110...|  0.0|[1.53712546825408...|       0.0|[0.82304644584655...|\n",
      "|[41.0,1.0,2.0,135...|  0.0|[2.32624101638793...|       0.0|[0.91102713346481...|\n",
      "|[41.0,1.0,3.0,130...|  0.0|[5.34667444229126...|       0.0|[0.99525862932205...|\n",
      "|[42.0,1.0,3.0,120...|  0.0|[-2.1023206710815...|       1.0|[0.10887145996093...|\n",
      "|[43.0,1.0,4.0,110...|  0.0|[-1.4359070062637...|       1.0|[0.19217997789382...|\n",
      "|[44.0,0.0,3.0,118...|  0.0|[3.24635004997253...|       0.0|[0.96254175901412...|\n",
      "|[44.0,1.0,2.0,120...|  0.0|[-0.9643662571907...|       1.0|[0.27600485086441...|\n",
      "|[44.0,1.0,3.0,140...|  0.0|[4.52281665802001...|       0.0|[0.98925822973251...|\n",
      "|[45.0,1.0,4.0,104...|  0.0|[-0.6661726236343...|       1.0|[0.33935439586639...|\n",
      "|[48.0,1.0,4.0,130...|  1.0|[-4.1037216186523...|       1.0|[0.01624292135238...|\n",
      "|[49.0,1.0,3.0,120...|  1.0|[-2.9739027023315...|       1.0|[0.04861885309219...|\n",
      "|[50.0,0.0,3.0,120...|  0.0|[6.85930728912353...|       0.0|[0.99895143508911...|\n",
      "|[51.0,0.0,4.0,130...|  1.0|[-3.1615219116210...|       1.0|[0.04063969850540...|\n",
      "|[52.0,0.0,3.0,136...|  0.0|[7.45881748199462...|       0.0|[0.99942398071289...|\n",
      "+--------------------+-----+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_df_test_vetor.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e60ee3b1-5f04-412d-a05f-b496d5a644b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier f1=0.651299464542249\n"
     ]
    }
   ],
   "source": [
    "## Metrica do resultado para classificação\n",
    "classifier_evaluator = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
    "print(f\"classifier f1={classifier_evaluator.evaluate(transformed_df_test_vetor)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "7fef5784-c2fc-4830-b7dd-e31d88419e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------------------+\n",
      "|            features|label|ValidationIndicatorCol|\n",
      "+--------------------+-----+----------------------+\n",
      "|[29.0,1.0,2.0,130...|  0.0|                  true|\n",
      "|[34.0,0.0,2.0,118...|  0.0|                 false|\n",
      "|[35.0,0.0,4.0,138...|  0.0|                  true|\n",
      "|[35.0,1.0,2.0,122...|  0.0|                 false|\n",
      "|[35.0,1.0,4.0,120...|  1.0|                 false|\n",
      "+--------------------+-----+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Criando um segundo modelo com a coluna de ValidationIndicatorCol a mais\n",
    "\n",
    "df_train_vetor_2 = df_train_vetor.withColumn(\"ValidationIndicatorCol\",rand(random_state) >0.7)\n",
    "df_train_vetor_2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8679c8e-e363-4e9a-83f9-13297e6aa1d5",
   "metadata": {},
   "source": [
    "**validationIndicatorCol:** Esta opção é usada para especificar a coluna de indicação de validação. Essa coluna é usada para identificar os registros que devem ser incluídos no conjunto de validação durante o treinamento do modelo. Geralmente, você divide seu conjunto de dados em conjuntos de treinamento e validação para avaliar o desempenho do modelo durante o treinamento. A coluna de indicação de validação é usada para indicar quais registros pertencem ao conjunto de validação. Se fornecido, o modelo usará essa coluna durante o treinamento para separar os dados em conjuntos de treinamento e validação. Se não for fornecido, o modelo usará uma divisão aleatória padrão.\n",
    "\n",
    "**weightCol:** Esta opção é usada para especificar a coluna que contém os pesos associados a cada amostra. Os pesos são usados para fornecer importância diferenciada às amostras durante o treinamento do modelo. Por exemplo, se algumas amostras são mais importantes ou mais representativas do que outras, você pode atribuir pesos mais altos a essas amostras para que o modelo leve isso em consideração durante o treinamento. Se não for fornecido, todos os pesos são considerados iguais.\n",
    "\n",
    "**base_margin_col:** Pode usar essa coluna como base_margin_col no modelo XGBoost para fornecer ao modelo uma pontuação inicial para cada cliente, refletindo sua confiança. Durante o treinamento, o modelo então ajustaria esses valores de margem base para otimizar a previsão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "010b54fb-111a-462d-8a6a-28edaffcac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invocar momdelo indicando coluna de validação do indicador\n",
    "xgb_classifier2 = SparkXGBClassifier(\n",
    "    max_depth=5, validation_indicator_col=\"ValidationIndicatorCol\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "68dd983b-bde7-4972-a466-4f84afbb146d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-13 22:48:28,721 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'device': 'cpu', 'max_depth': 5, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-02-13 22:48:31,793 INFO XGBoost-PySpark: _fit Finished xgboost training!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------------------+--------------------+----------+--------------------+\n",
      "|            features|label|ValidationIndicatorCol|       rawPrediction|prediction|         probability|\n",
      "+--------------------+-----+----------------------+--------------------+----------+--------------------+\n",
      "|[29.0,1.0,2.0,130...|  0.0|                 false|[6.67216920852661...|       0.0|[0.99873596429824...|\n",
      "|[34.0,0.0,2.0,118...|  0.0|                 false|[6.46621751785278...|       0.0|[0.99844729900360...|\n",
      "|[35.0,0.0,4.0,138...|  0.0|                 false|[5.10124015808105...|       0.0|[0.99394768476486...|\n",
      "|[35.0,1.0,2.0,122...|  0.0|                 false|[5.42273092269897...|       0.0|[0.99560433626174...|\n",
      "|[35.0,1.0,4.0,120...|  1.0|                  true|[-4.6494755744934...|       1.0|[0.00947600603103...|\n",
      "|[38.0,1.0,1.0,120...|  1.0|                  true|[-0.9622104167938...|       1.0|[0.27643585205078...|\n",
      "|(13,[0,2,3,4,7,10...|  0.0|                 false|[5.71387100219726...|       0.0|[0.99671095609664...|\n",
      "|(13,[0,2,3,4,7,10...|  0.0|                 false|[5.53918647766113...|       0.0|[0.99608564376831...|\n",
      "|[39.0,1.0,3.0,140...|  0.0|                 false|[3.74049353599548...|       0.0|[0.97680824995040...|\n",
      "|[39.0,1.0,4.0,118...|  1.0|                 false|[-3.7281486988067...|       1.0|[0.02347302436828...|\n",
      "|[40.0,1.0,1.0,140...|  0.0|                 false|[3.00145959854125...|       0.0|[0.95263999700546...|\n",
      "|[40.0,1.0,4.0,110...|  1.0|                 false|[-5.1962924003601...|       1.0|[0.00550651550292...|\n",
      "|[40.0,1.0,4.0,152...|  1.0|                 false|[-1.8560886383056...|       1.0|[0.13515955209732...|\n",
      "|[41.0,0.0,2.0,105...|  0.0|                  true|[3.24539136886596...|       0.0|[0.96250712871551...|\n",
      "|[41.0,0.0,3.0,112...|  0.0|                  true|[6.1876540184021,...|       0.0|[0.99794960021972...|\n",
      "|[41.0,1.0,2.0,120...|  0.0|                  true|[5.19309616088867...|       0.0|[0.99447590112686...|\n",
      "|[41.0,1.0,3.0,112...|  0.0|                  true|[4.00225448608398...|       0.0|[0.98205357789993...|\n",
      "|[41.0,1.0,4.0,110...|  1.0|                  true|[0.73261469602584...|       0.0|[0.67537879943847...|\n",
      "|(13,[0,2,3,4,7,10...|  0.0|                 false|[6.08620500564575...|       0.0|[0.99773114919662...|\n",
      "|[42.0,0.0,4.0,102...|  0.0|                 false|[3.33701729774475...|       0.0|[0.96567714214324...|\n",
      "+--------------------+-----+----------------------+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "classifier2 f1=0.9582809553739786\n"
     ]
    }
   ],
   "source": [
    "# Treinamento novamente do modelo\n",
    "xgb_classifier_model2 = xgb_classifier2.fit(df_train_vetor_2)\n",
    "transformed_df_train_vetor_2 = xgb_classifier_model2.transform(df_train_vetor_2)\n",
    "transformed_df_train_vetor_2.show()\n",
    "print(\n",
    "    f\"classifier2 f1={classifier_evaluator.evaluate(transformed_df_train_vetor_2)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f8e5be06-761c-49b2-9965-e0d995939b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---+-------------+\n",
      "|ValidationIndicatorCol|Qtd|Proporção (%)|\n",
      "+----------------------+---+-------------+\n",
      "|                  true| 68|         0.28|\n",
      "|                 false|172|         0.72|\n",
      "+----------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Na ValidationIndicatorCol, o valor True são os considerados ara validação e False para treinamento\n",
    "transformed_df_train_vetor_2.groupBy(col(\"ValidationIndicatorCol\")) \\\n",
    "                            .agg(count(\"*\").alias(\"Qtd\"), \n",
    "                                round(count(\"*\")/ transformed_df_train_vetor_2.count()\n",
    "                                      ,2).alias(\"Proporção (%)\")\n",
    "                                ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "632d6ab2-2a7e-4358-9d3e-d5cafdf5612d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='SparkXGBClassifier_06594260c6e3', name='enable_sparse_data_optim', doc='This stores the boolean config of enabling sparse data optimization, if enabled, Xgboost DMatrix object will be constructed from sparse matrix instead of dense matrix. This config is disabled by default. If most of examples in your training dataset contains sparse features, we suggest to enable this config.'): False,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='featuresCol', doc='features column name.'): 'features',\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='features_cols', doc='feature column names.'): [],\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='labelCol', doc='label column name.'): 'label',\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability',\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction',\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='arbitrary_params_dict', doc='arbitrary_params_dict This parameter holds all of the additional parameters which are not exposed as the the XGBoost Spark estimator params but can be recognized by underlying XGBoost library. It is stored as a dictionary.'): {},\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='base_score', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param base_score'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='booster', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param booster'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='callbacks', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier.fit() for this param callbacksThe callbacks can be arbitrary functions. It is saved using cloudpickle which is not a fully self-contained format. It may fail to load with different versions of dependencies.'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='colsample_bylevel', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param colsample_bylevel'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='colsample_bynode', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param colsample_bynode'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='colsample_bytree', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param colsample_bytree'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='device', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param device'): 'cpu',\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='early_stopping_rounds', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier.fit() for this param early_stopping_rounds'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='eval_metric', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier.fit() for this param eval_metric'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='feature_names', doc='A list of str to specify feature names.'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='feature_types', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param feature_types'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='feature_weights', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier.fit() for this param feature_weights'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='force_repartition', doc='A boolean variable. Set force_repartition=true if you want to force the input dataset to be repartitioned before XGBoost training.Note: The auto repartitioning judgement is not fully accurate, so it is recommendedto have force_repartition be True.'): False,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='gamma', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param gamma'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='grow_policy', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param grow_policy'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='importance_type', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param importance_type'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='interaction_constraints', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param interaction_constraints'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='iteration_range', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier.predict() for this param iteration_range'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='learning_rate', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param learning_rate'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='max_bin', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param max_bin'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='max_cat_threshold', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param max_cat_threshold'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='max_cat_to_onehot', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param max_cat_to_onehot'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='max_delta_step', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param max_delta_step'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='max_depth', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param max_depth'): 5,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='max_leaves', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param max_leaves'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='min_child_weight', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param min_child_weight'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='missing', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param missing'): nan,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='monotone_constraints', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param monotone_constraints'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='multi_strategy', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param multi_strategy'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='n_estimators', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param n_estimators'): 100,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='num_parallel_tree', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param num_parallel_tree'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='num_workers', doc='The number of XGBoost workers. Each XGBoost worker corresponds to one spark task.'): 1,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='objective', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param objective'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='random_state', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param random_state'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='reg_alpha', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param reg_alpha'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='reg_lambda', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param reg_lambda'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='repartition_random_shuffle', doc='A boolean variable. Set repartition_random_shuffle=true if you want to random shuffle dataset when repartitioning is required. By default is True.'): False,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='sampling_method', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param sampling_method'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='scale_pos_weight', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param scale_pos_weight'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='subsample', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param subsample'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='tree_method', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param tree_method'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='use_gpu', doc='Deprecated, use `device` instead. A boolean variable. Set use_gpu=true if the executors are running on GPU instances. Currently, only one GPU per task is supported.'): False,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='validate_parameters', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param validate_parameters'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='verbose', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier.fit() for this param verbose'): True,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='verbosity', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param verbosity'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='xgb_model', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier.fit() for this param xgb_model'): None,\n",
       " Param(parent='SparkXGBClassifier_06594260c6e3', name='validationIndicatorCol', doc='name of the column that indicates whether each row is for training or for validation. False indicates training; true indicates validation.'): 'ValidationIndicatorCol'}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xgb_classifier_model2.explainParams()\n",
    "xgb_classifier_model2.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "5d11f8df-a339-4e4d-83c3-3e16cc1b4bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinameto com validação cruzada e pipeline\n",
    "\n",
    "xgb_classifier_cross_val = SparkXGBClassifier(\n",
    "    objetive=\"binary:logistic\",\n",
    "    # validation_indicator_col=\"ValidationIndicatorCol\"\n",
    "    num_round=100,\n",
    "    verbose = 1,\n",
    "    # max_depth=5, \n",
    "    # nthread=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "6ffbda56-117c-4f12-8931-fad0ceb0f96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(xgb_classifier_cross_val.max_depth,[3, 6, 9])\n",
    "             .addGrid(xgb_classifier_cross_val.learning_rate,[0.1, 0.3,0.5])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "0de7f91d-0c47-4cb8-8c2f-3eda670593f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "f97cca8b-d045-45b8-af62-f94775f3b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CrossValidator(estimator = xgb_classifier_cross_val,\n",
    "                  estimatorParamMaps = paramGrid,\n",
    "                  evaluator = evaluator,\n",
    "                  numFolds = 2\n",
    "                  seed = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "64b03016-9814-41bc-bd63-34e50329ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "c38c011f-69eb-440e-b7df-ca18b071aa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-14 14:29:01,434 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n",
      "\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'objetive': 'binary:logistic', 'num_round': 100, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': 1, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-02-14 14:29:04,424 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-02-14 14:29:04,755 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n",
      "\tbooster params: {'device': 'cpu', 'learning_rate': 0.3, 'max_depth': 3, 'objective': 'binary:logistic', 'objetive': 'binary:logistic', 'num_round': 100, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': 1, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-02-14 14:29:07,011 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-02-14 14:29:07,302 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n",
      "\tbooster params: {'device': 'cpu', 'learning_rate': 0.5, 'max_depth': 3, 'objective': 'binary:logistic', 'objetive': 'binary:logistic', 'num_round': 100, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': 1, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-02-14 14:29:09,553 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-02-14 14:29:09,898 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n",
      "\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'objetive': 'binary:logistic', 'num_round': 100, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': 1, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-02-14 14:29:12,254 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-02-14 14:29:12,538 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n",
      "\tbooster params: {'device': 'cpu', 'learning_rate': 0.3, 'max_depth': 6, 'objective': 'binary:logistic', 'objetive': 'binary:logistic', 'num_round': 100, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': 1, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-02-14 14:29:14,794 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-02-14 14:29:15,074 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n",
      "\tbooster params: {'device': 'cpu', 'learning_rate': 0.5, 'max_depth': 6, 'objective': 'binary:logistic', 'objetive': 'binary:logistic', 'num_round': 100, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': 1, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-02-14 14:29:17,354 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-02-14 14:29:17,631 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n",
      "\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'objetive': 'binary:logistic', 'num_round': 100, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': 1, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-02-14 14:29:19,887 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-02-14 14:29:20,168 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n",
      "\tbooster params: {'device': 'cpu', 'learning_rate': 0.3, 'max_depth': 9, 'objective': 'binary:logistic', 'objetive': 'binary:logistic', 'num_round': 100, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': 1, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-02-14 14:29:22,423 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-02-14 14:29:22,716 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n",
      "\tbooster params: {'device': 'cpu', 'learning_rate': 0.5, 'max_depth': 9, 'objective': 'binary:logistic', 'objetive': 'binary:logistic', 'num_round': 100, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': 1, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-02-14 14:29:24,975 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-02-14 14:29:25,323 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n",
      "\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'objetive': 'binary:logistic', 'num_round': 100, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': 1, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-02-14 14:29:27,574 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-02-14 14:29:27,938 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n",
      "\tbooster params: {'device': 'cpu', 'learning_rate': 0.3, 'max_depth': 3, 'objective': 'binary:logistic', 'objetive': 'binary:logistic', 'num_round': 100, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': 1, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-02-14 14:29:30,188 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-02-14 14:29:30,442 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n",
      "\tbooster params: {'device': 'cpu', 'learning_rate': 0.5, 'max_depth': 3, 'objective': 'binary:logistic', 'objetive': 'binary:logistic', 'num_round': 100, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': 1, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-02-14 14:29:32,693 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-02-14 14:29:32,990 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n",
      "\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'objetive': 'binary:logistic', 'num_round': 100, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': 1, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-02-14 14:29:35,248 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-02-14 14:29:35,539 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n",
      "\tbooster params: {'device': 'cpu', 'learning_rate': 0.3, 'max_depth': 6, 'objective': 'binary:logistic', 'objetive': 'binary:logistic', 'num_round': 100, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': 1, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-02-14 14:29:37,805 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-02-14 14:29:38,123 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n",
      "\tbooster params: {'device': 'cpu', 'learning_rate': 0.5, 'max_depth': 6, 'objective': 'binary:logistic', 'objetive': 'binary:logistic', 'num_round': 100, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': 1, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-02-14 14:29:40,395 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-02-14 14:29:40,663 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n",
      "\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'objetive': 'binary:logistic', 'num_round': 100, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': 1, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-02-14 14:29:42,941 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-02-14 14:29:43,237 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n",
      "\tbooster params: {'device': 'cpu', 'learning_rate': 0.3, 'max_depth': 9, 'objective': 'binary:logistic', 'objetive': 'binary:logistic', 'num_round': 100, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': 1, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-02-14 14:29:45,504 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-02-14 14:29:45,788 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n",
      "\tbooster params: {'device': 'cpu', 'learning_rate': 0.5, 'max_depth': 9, 'objective': 'binary:logistic', 'objetive': 'binary:logistic', 'num_round': 100, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': 1, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-02-14 14:29:48,051 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-02-14 14:29:48,318 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n",
      "\tbooster params: {'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'objetive': 'binary:logistic', 'num_round': 100, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': 1, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-02-14 14:29:50,619 INFO XGBoost-PySpark: _fit Finished xgboost training!\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(df_train_vetor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "059f7843-f753-41bf-bb39-7d2357683500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_95f100facd4c"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "63843afe-dc84-4f7b-baf2-7199b1fc7543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafb12fd-9c28-47cc-b6ee-7c0db96ba985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
